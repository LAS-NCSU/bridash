{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocoding of GeoAid Data V2\n",
    "\n",
    "The original geoaid data data includes a git repo of shapefiles that are linked in a csv to project descriptions. Since only 30% of this data was geocoded, we need to find additional references to locations to geocode. We use city (lat, long) coordinates and country/continental shapefiles. This is done iterativly to provide the most accurate granular location for a particular financial expenditure. (ie, only if a city cannot be found would a project be geocoded at the country level). \n",
    "\n",
    "Expansions in this line of work would include: \n",
    "- automation of geocoding \n",
    "- more granular locations identified \n",
    "\n",
    "Geocoded data appears as such: \n",
    "\n",
    "*Public Opinion Data Set*\n",
    "\n",
    "|Data Entity ID   |   Entity Attribute Fields  | Geocode Level 1 (country) ID | Geocode Level 2 (regional) ID  | Geocode Level 3 (Lat/Long) ID  | \n",
    "|---------|-----------------|--------------|---|-----------------|\n",
    "|Respondant_1 |   age/gender/opinions/... | 100 | 1001 | 100101  | \n",
    "|Respondant_2  |    age/gender/opinions/... | 200 | 2001 | 2001001  | \n",
    "\n",
    "*Geocoded Data*\n",
    "\n",
    "| Geocode Level   |   Name  | ID | geometry | Attribute Fields  |\n",
    "|---------|-------| -----|-----|--------|\n",
    "|1  |  USA | 100 | MultiPolygon(...) | gdp/bri_partnership/wealth/access/... | \n",
    "|3  |  Siegen | 2001 | Point(...)        | gdp/bri_partnership/wealth/access/... |\n",
    "|2  |  Noth Rhine-Westphalia | 2001001 | MultiPolygon(...)  |  gdp/bri_partnership/wealth/access/... |\n",
    "\n",
    "After entities are geocoded, we convert to a csv and then export for use in ArcGIS. \n",
    "\n",
    "__More information on geocoding conventions and processes for adding data to the location tables can be found [here](https://github.ncsu.edu/nakraft/LAS-BRI/blob/master/data_final/geocode.md).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import osmapi as osm\n",
    "import regex as re\n",
    "\n",
    "# to retrieve geojson files from dataframe \n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import geojson\n",
    "from geojson import Feature, FeatureCollection\n",
    "import geopandas as gdp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the location of the git repo for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_repo_loc = os.path.dirname(os.path.realpath(\"AidDataV2_Parsing.ipynb\"))\n",
    "temp_directory = os.path.join(os.path.join(os.path.expanduser('~')), 'Desktop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Needed Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(path): \n",
    "    file = open(path, \"r\")\n",
    "    contents = file.read()\n",
    "    dictionary = json.loads(contents)\n",
    "    file.close()\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set collection\n",
    "\n",
    "There are three different datasets available for collection. \n",
    "- df: features development, commercial, representational or mixed intent \n",
    "- mil: features military financial expenditures \n",
    "- hu: features huawei financial expenditures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_excel(git_repo_loc + \"/AidDatasGlobalChineseDevelopmentFinanceDataset_v2.0.xlsx\", sheet_name='Global_CDF2.0')\n",
    "dev['data'] = 'Development'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mil = pd.read_excel(git_repo_loc + \"/AidDatasGlobalChineseDevelopmentFinanceDataset_v2.0.xlsx\", sheet_name='Military')\n",
    "mil['data'] = 'Military'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hu = pd.read_excel(git_repo_loc + \"/AidDatasGlobalChineseDevelopmentFinanceDataset_v2.0.xlsx\", sheet_name='Huawei')\n",
    "hu['data'] = 'Huawei'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data collected has synonymous fields, with the exception if they are good for military, huawei, or developmental aggregates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([dev, mil, hu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['aggregate'] = (df['Recommended For Aggregates'].fillna(\"No\").map({\"Yes\":1, \"No\":0}) + \n",
    "                   df['Recommended For Military Aggregates'].fillna(\"No\").map({\"Yes\":1, \"No\":0}) + \n",
    "                   df['Recommended For Huawei Aggregates'].fillna(\"No\").map({\"Yes\":1, \"No\":0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These values describe the count of projects to be aggregated moving forward.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    11394\n",
       "0     2644\n",
       "Name: aggregate, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These values describe the count of projects to be aggregated moving forward.\")\n",
    "df['aggregate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the names to a common value for searching for the geometry \n",
    "# grabs it from txt file for consistency of country naming conventions\n",
    "\n",
    "recipient_mapping = load_dict(\"../country_config.txt\")\n",
    "df['Recipient'] = df['Recipient'].replace(recipient_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wk = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining our geospatial capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 27.87431981744778% projects already geocoded in the dataset.\n",
      "We can geocode the rest of the data using our geocoding process.\n"
     ]
    }
   ],
   "source": [
    "none_loc = df.loc[(df['geoJSON URL DL'].notna()) & (df['aggregate'] == 1)] \n",
    "print(\"There is \" + str(none_loc['AidData TUFF Project ID'].count() * 100 / len(df[df['aggregate'] == 1])) + \"% projects already geocoded in the dataset.\")\n",
    "print(\"We can geocode the rest of the data using our geocoding process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-994bc327a545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mwk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AidData TUFF Project ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeocode_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AidData TUFF Project ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeocode_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"There are still \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" more entities to geocode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pull in the mapping of files with geocode available\n",
    "geocode_map = pd.read_csv(git_repo_loc + \"/geoaid_data_to_loc_mapping.csv\").drop(columns='Unnamed: 0')\n",
    "wk = wk[~wk['AidData TUFF Project ID'].isin(geocode_map['AidData TUFF Project ID'])].reset_index()\n",
    "\n",
    "assert len(wk) == (len(df) - len(geocode_map))\n",
    "print(\"There are still \" + str(len(wk)) + \" more entities to geocode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use wk as a dataframe of non-geocoded entities\n",
    "\n",
    "Narrow down the 'wk' to include key location data.\n",
    "\n",
    "__Let's first geocode anything that has explicit url demonstrating a location__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wk = wk[['AidData TUFF Project ID', 'Recipient', 'Recipient Region', 'Geographic Location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = osm.OsmApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wk['urls'] = \"\"\n",
    "for inde, element in enumerate((wk['Geographic Location'].astype(str))):\n",
    "    mask = (pd.notna(element)) and ((\"openstreetmap\" in element) or (\"google.com/maps\" in element))\n",
    "    wk['urls'][inde] = [x for x in element.split(\" \") if (\"openstreetmap\" in x) or (\"google.com/maps\" in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = wk.loc[(wk['urls'] != \"\") & (wk['urls'].str.len() != 0)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_coordinates_from_url(url, api): \n",
    "    \n",
    "    if \"https://www.openstreetmap.org/node\" == url[:34]: \n",
    "        x = re.findall(r\"https://www.openstreetmap.org/node/(.*)\", url)[0]\n",
    "        j = api.NodeGet(x)\n",
    "        lat = j['lat']\n",
    "        long = j['lon']\n",
    "        try: \n",
    "            name = j['tag']['name']\n",
    "        except: \n",
    "            name = \"\"\n",
    "        return [name, lat, long]\n",
    "    \n",
    "    if \"https://www.openstreetmap.org/way/\" == url[:34]: \n",
    "        try: \n",
    "            x = re.findall(r\"https://www.openstreetmap.org/way/(.*)\", url)[0]\n",
    "            j = api.WayGet(x)\n",
    "            nodes = j['nd']\n",
    "            loc = api.NodeGet(nodes[0])\n",
    "            lat = loc['lat']\n",
    "            long = loc['lon']\n",
    "            try: \n",
    "                name = nodes['tag']['name']\n",
    "            except: \n",
    "                name = \"\"\n",
    "            return [name, lat, long]\n",
    "        except: \n",
    "            pass\n",
    "            #nothing happens\n",
    "    \n",
    "    r = {\n",
    "        \"https://www.openstreetmap.org/qu\" : r\"(.*)https://www.openstreetmap.org/query\\?lat=(.*)&lon=(.*)\", \n",
    "        \"https://www.openstreetmap.org/wa\" : r'https://www.openstreetmap.org/way/(.*)\\#map=(?:.*)\\/(.*)\\/(.*)',\n",
    "        \"https://www.openstreetmap.org/re\" : r\"https://www.openstreetmap.org/relation/(.*)\\#map=[\\d]/(.*)/(.*)\",\n",
    "        \"https://www.openstreetmap.org/se\" : r\"(.*)https://www.openstreetmap.org/search\\?query=(?:.*)\\#map=(?:.*)\\/(.*)\\/(.*)\",\n",
    "        \"https://www.google.com/maps/plac\" : r\"https://www.google.com/maps/place/(.*)/@(.*),(.*),(?:.*)data=(?:.*)\", \n",
    "        \"https://www.google.com/maps/dir/\" : r\"https://www.google.com/maps/dir/(.*)/@(.*),(.*),\", \n",
    "        \"https://www.google.com/maps/d/u/\" : r\"(.*)https://www.google.com/maps/d/u/0/edit(?:.*)ll=(.*)\\%2C(.*)&z=(?:.*)\",  \n",
    "    }\n",
    "    \n",
    "  #  \"https://www.google.com/maps/dir/Utexrwa,+KG+15+Ave,+Kigali,+Rwanda/Kinyinya,+Kigali,+Rwanda/\n",
    "  #  @-1.9215882,30.0667421,14z/data=!3m1!4b1!4m14!4m13!1m5!1m1!1s0x19dca6adc81ee515:0xe7149cb605da8846!2m2!1d30.075835!2d-1.9277261!1m5!1m1!1s0x19dca13db5bbf103:0xce704dade862c428!2m2!1d30.0954076!2d-1.9113692!3e0\"\n",
    "    # determine how to parse the data \n",
    "    # t will correspond to (name, lat, long) or (id, lat, long), where id can be parsed to get the name\n",
    "    try: \n",
    "        regex = r[url[:32]]\n",
    "    except: \n",
    "        # print(\"the \" + str(url) + \" was not matched.\")\n",
    "        # nothing occurs\n",
    "        pass\n",
    "        \n",
    "    try:  \n",
    "        t = list(re.findall(regex, url)[0])\n",
    "        t[0] = t[0].replace(\"+\", \" \")\n",
    "        return t  \n",
    "    except: \n",
    "        # print(\"the regex \" + regex + \" had issues with url \" + url)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    }
   ],
   "source": [
    "urls['coord'] = [list() for x in range(len(urls))]\n",
    "for i in range(0, len(urls)): \n",
    "    for e in range(0, len(urls['urls'][i])): \n",
    "        t = find_coordinates_from_url(urls['urls'][i][e], api)\n",
    "        if t is not None: \n",
    "            urls['coord'][i].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls['latitude'] = \"\"\n",
    "urls['longitude'] = \"\"\n",
    "urls['location_name'] = \"\"\n",
    "\n",
    "for i in range(0, len(urls)): \n",
    "    # if the array is of length 1, use it as the location \n",
    "    # if it has multiple, where there is only 1 with an actual location name, use that as the location\n",
    "    # if it has multiple, where they is more than 1 with a location name, use the project id and use the average of coordinates for point\n",
    "    # if there is multiple but none of them have a name, average all the locations for the point \n",
    "    # if no locations identified, print error message \n",
    "    \n",
    "    if (len(urls['coord'][i]) == 0) | (urls['coord'][i] == [\"\"]):\n",
    "        continue\n",
    "        \n",
    "    if len(urls['coord'][i]) == 1: \n",
    "        if urls['coord'][i][0][0] == \"\": \n",
    "            urls['location_name'][i] = \"\"\n",
    "        else: \n",
    "            urls['location_name'][i] = urls['coord'][i][0][0]\n",
    "        urls['latitude'][i] = urls['coord'][i][0][1]\n",
    "        urls['longitude'][i] = urls['coord'][i][0][2]\n",
    "        continue \n",
    "    \n",
    "    if len(urls['coord'][i]) > 1: \n",
    "        temp = urls['coord'][i]\n",
    "        first_ele = [x[0] for x in urls['coord'][i]]\n",
    "        if first_ele.count(\"\") == len(first_ele) - 1: \n",
    "            loc = [x for x in urls['coord'][i] if x[0] != \"\"][0]\n",
    "            urls['location_name'][i] = loc[0]\n",
    "            urls['latitude'][i] = loc[1]\n",
    "            urls['longitude'][i] = loc[2]\n",
    "            \n",
    "        else: \n",
    "            urls['location_name'][i] = \"\"\n",
    "            urls['latitude'][i] = np.mean([float(x[1]) for x in urls['coord'][i]])\n",
    "            urls['longitude'][i] = np.mean([float(x[2]) for x in urls['coord'][i]])\n",
    "\n",
    "# drop all empty data\n",
    "urls = urls[urls['location_name'] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls['location_name'] = [re.sub('(\\%[a-zA-Z0-9]{2})', '', x).split(\",\")[0] for x in urls['location_name'].astype(str)]\n",
    "urls['location_name'] = [re.sub(\"\\d{1,3}'\\d{2}.\\d[NESW]\", \"\", x).strip() for x in urls['location_name'].astype(str)]\n",
    "urls['location_name'] = [re.sub(\"[NESW]\\d\", \"\", x) for x in urls['location_name'].astype(str)]\n",
    "urls['location_name'] = [re.sub('[0-9] [0-9]', '', x) for x in urls['location_name']]\n",
    "urls['location_name'] = [re.sub('[0-9]', '', x) if x.isdigit() else x for x in urls['location_name']]\n",
    "\n",
    "# now we have coordinates. These need to go to geocoder to go from coordinates to location \n",
    "loc_data = temp_directory + \"/city_reverse_geocoding_temp.csv\"\n",
    "urls = urls[['AidData TUFF Project ID', 'Recipient', 'latitude', 'longitude', 'location_name']]\n",
    "urls.to_csv(loc_data, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can run our geocoding through a unique script. This will ensure consistency across other entity geocoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/natalie_kraft/Documents/LAS/LAS-BRI/data_processing\n",
      "Preparing system configuration.\n",
      "Loading file to geocode\n",
      "You are reverse geocoding cities. Begin geocoding.\n",
      "Loading geocoded location entities.\n",
      "Loading geocoded location entities.\n",
      "Loading formatted geocoded file...\n",
      "\tfound Centro Cultural Chins with id 63\n",
      "\tfound 53281.0, near Outapi, NA with id 28\n",
      "\tfound Levy Mwanawasa General Hospital with id 59\n",
      "\tfound Levy Mwanawasa General Hospital with id 59\n",
      "\tThis country Africa was not found\n",
      "\tThe country Africa will not be added to the listing.\n",
      "\tfound Mariel with id 50\n",
      "\tfound 30481.0, near Sao Tome, ST with id 39\n",
      "\tfound 73057.0, near Vestmannaeyjar, IS with id 9\n",
      "\tfound 58671 near Aplahoue, BJ with id 51\n",
      "\tfound Levy Mwanawasa General Hospital with id 59\n",
      "\tfound Maina Soko Military Hospital with id 61\n",
      "\tfound 58671 near Aplahoue, BJ with id 51\n",
      "\tfound 30629.0, near Yandev, NG with id 86\n",
      "\tfound Centre Hospitalier National de Pikine with id 32\n",
      "\tfound Maina Soko Military Hospital with id 61\n",
      "\tfound Maamba Collieries Limited with id 62\n",
      "\tfound Kaunda Square Secondary School with id 60\n",
      "\tfound Shahjibazar 330 MW Power Plant with id 58\n",
      "\tfound 59349 near Iguape, BR with id 3418\n",
      "\tfound 59349 near Iguape, BR with id 3418\n",
      "\tfound 59349 near Iguape, BR with id 3418\n",
      "\tfound 59349 near Iguape, BR with id 3418\n",
      "\tfound 73057.0, near Vestmannaeyjar, IS with id 9\n",
      "\tfound 40735 near Chimoio, MZ with id 64\n",
      "\tfound Centre Hospitalier National de Pikine with id 32\n",
      "\tfound Maina Soko Military Hospital with id 61\n",
      "\tfound Stade des Martyrs with id 146\n",
      "\tfound 40735 near Chimoio, MZ with id 64\n",
      "\tfound Centre Hospitalier National de Pikine with id 32\n",
      "\tfound Centre Hospitalier National de Pikine with id 32\n",
      "\tfound Maina Soko Military Hospital with id 61\n",
      "\tfound 30600.0, near Pale, GQ with id 40\n",
      "\tfound 73057.0, near Vestmannaeyjar, IS with id 9\n",
      "\tfound culture centre of China with id 92\n",
      "\tfound 55684.0, near Ayr, GB with id 5\n",
      "\tfound Les Mamelles with id 20\n",
      "\tfound Maina Soko Military Hospital with id 61\n",
      "\tfound Centre Hospitalier National de Pikine with id 32\n",
      "\tfound Les Mamelles with id 20\n",
      "\tfound Les Mamelles with id 20\n",
      "\tfound 55684.0, near Ayr, GB with id 5\n",
      "\tfound Les Mamelles with id 20\n",
      "\tfound Levy Mwanawasa General Hospital with id 59\n",
      "\tfound Chitungwiza Central Hospital with id 65\n",
      "\tfound Chitungwiza Central Hospital with id 65\n",
      "\tfound Campamento del Proyecto Minero Mirador with id 101\n",
      "\tfound cole Primaire Publique De L'amiti Sino-Gabonaise with id 52\n",
      "\tfound 65064.0, near Sao Tome, ST with id 19\n",
      "\tfound Maina Soko Military Hospital with id 61\n",
      "\tfound Mahusekwa Hospital with id 61\n",
      "\tfound Mahusekwa Hospital with id 61\n",
      "\tfound Hospital Regional de Cacheu with id 27\n",
      "\tfound CCI IVATO with id 55\n",
      "\tfound CCI IVATO with id 55\n",
      "\tfound Centre Hospitalier National de Pikine with id 32\n",
      "\tfound Centre Hospitalier National de Pikine with id 32\n",
      "\tfound 59154.0, near Las Palmas de Gran Canaria, ES with id 38\n",
      "\tfound Levy Mwanawasa General Hospital with id 59\n",
      "\tfound Chainama Hills College Hospital with id 67\n",
      "\tfound Mahusekwa Hospital with id 61\n",
      "\tfound Hospital General De Mpanda with id 33\n",
      "\tfound Centro Cultural Chins with id 63\n",
      "\tfound 55684.0, near Ayr, GB with id 5\n",
      "\tfound Hospital Prince Rgent Charles with id 29\n",
      "\tfound Bujumbura with id 2\n",
      "\tfound Centre Hospitalier National de Pikine with id 32\n",
      "\tfound Kigali with id 10\n",
      "\tfound Hwange Colliery Company with id 63\n",
      "\tfound Hwange Colliery Company with id 63\n",
      "\tfound Ecole Technique Professionel de Kigobe with id 30\n",
      "\tfound Hospital Prince Rgent Charles with id 29\n",
      "\tfound CCI IVATO with id 55\n",
      "\tfound Les Mamelles with id 20\n",
      "\tfound Moroni with id 4\n",
      "\tfound Les Mamelles with id 20\n",
      "\tfound Sino-Zimbabwe Cement Company (Plant) with id 66\n",
      "\tfound Hospital Regional de Cacheu with id 27\n",
      "\tfound 178 near Kuraymah, SD with id 83\n",
      "\tfound 178 near Kuraymah, SD with id 83\n",
      "\tfound Hwange Colliery Company with id 63\n",
      "\tfound Bairro dos Antigos Combatentes with id 29\n",
      "\tfound Central Eletrica de Guine-Bissau with id 30\n",
      "\tfound Kampala with id 49\n",
      "\tfound 59154.0, near Las Palmas de Gran Canaria, ES with id 38\n",
      "\tfound 55736.0, near Ayr, GB with id 17\n",
      "\tfound New Century Career Training Institute with id 70\n",
      "\tfound Sino-Zimbabwe Cement Company (Plant) with id 66\n",
      "\tfound 62952.0, near Ribeira Brava, PT with id 58\n",
      "\tfound Military School with id 56\n",
      "Exporting new cities to database\n",
      "Exporting modified data to geocoded location entities.\n",
      "Exporting mapping results.\n",
      "Geocoding complete.\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%run autogeocode.py /Users/natalie_kraft/Desktop/city_reverse_geocoding_temp.csv C location_name Recipient \"AidData TUFF Project ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(temp_directory + \"/city_reverse_geocoding_temp_results.csv\")\n",
    "geocode_map = geocode_map.append(results[results['city_temp_id'] != \"-1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are still 10527 more entities to geocode\n"
     ]
    }
   ],
   "source": [
    "# any result that has been geocoded, restrict from wk \n",
    "temp_wk = wk.merge(results, on='AidData TUFF Project ID', how =\"left\")\n",
    "temp_wk = temp_wk[(pd.isna(temp_wk['city_temp_id'])) | (temp_wk['city_temp_id'] == \"-1\")]\n",
    "wk = temp_wk.drop(columns='city_temp_id')\n",
    "\n",
    "assert len(wk) == (len(df) - len(geocode_map))\n",
    "print(\"There are still \" + str(len(wk)) + \" more entities to geocode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use wk as a dataframe of non-geocoded entities - no URL locations\n",
    "\n",
    "\n",
    "__Next we can geocode anything that has any city information present demonstrating a location__\n",
    "- inclusive of standard city naming conventions \n",
    "- and inclusive of \"town of\" and \"city of\" tags \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a city column for all of the cities listed in the \"Geographic Location\" columns \n",
    "# establishes more details on cities given the full description\n",
    "wk['city'] = [re.findall(\"(?:town|city|City|located|commune|University) (?:of|in) (?:[A-Z]\\w*(?:\\s|\\.|,))+\", str(x)) for x in wk['Geographic Location']]\n",
    "wk['city'] = [re.sub(\"(town|city|City|located) (of|in) \", \"\", x[0])[:-1] if len(x) !=0 else \"\" for x in wk['city']]\n",
    "\n",
    "# establishes based on \"located in\"\n",
    "# checks for standard city conventions, one word or all caps 'city, country'\n",
    "p = re.compile(\"(?:[A-Z]\\w*(?:\\s|\\.|,){0,2})+\")\n",
    "temp_city_list = [p.match(str(x)) for x in wk['Geographic Location']]\n",
    "temp_city_list = [x.group() if x != None else \"\" for x in temp_city_list]\n",
    "temp_city_list = [x if (\"This \" not in x) & (\"The \" not in x) & (\"There \" not in x) else \"\" for x in temp_city_list]\n",
    "\n",
    "wk['city'] = wk['city'] + temp_city_list \n",
    "\n",
    "export = wk[wk['city'] != \"\"][['AidData TUFF Project ID', 'Recipient', 'city']]\n",
    "\n",
    "loc_data = temp_directory + \"/city_geocoding_temp.csv\"\n",
    "export.to_csv(loc_data, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing system configuration.\n",
      "Loading file to geocode\n",
      "You are geocoding cities. Begin geocoding.\n",
      "Loading geocoded location entities.\n",
      "Loading geocoded location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country South America was not found\n",
      "Location South America not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Africa was not found\n",
      "Location Africa not added to mapping or location entities.\n",
      "\tThis country Africa was not found\n",
      "Location Africa not added to mapping or location entities.\n",
      "\tThis country South America was not found\n",
      "Location South America not added to mapping or location entities.\n",
      "\tThis country South America was not found\n",
      "Location South America not added to mapping or location entities.\n",
      "\tThis country Africa was not found\n",
      "Location Africa not added to mapping or location entities.\n",
      "\tThis country Africa was not found\n",
      "Location Africa not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Africa was not found\n",
      "Location Africa not added to mapping or location entities.\n",
      "\tThis country South America was not found\n",
      "Location South America not added to mapping or location entities.\n",
      "\tThis country South America was not found\n",
      "Location South America not added to mapping or location entities.\n",
      "\tThis country South America was not found\n",
      "Location South America not added to mapping or location entities.\n",
      "\tThis country Multi-Region was not found\n",
      "Location Multi-Region not added to mapping or location entities.\n",
      "\tThis country Multi-Region was not found\n",
      "Location Multi-Region not added to mapping or location entities.\n",
      "\tThis country Multi-Region was not found\n",
      "Location Multi-Region not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Africa was not found\n",
      "Location Africa not added to mapping or location entities.\n",
      "Exporting new cities to database\n",
      "Exporting modified data to geocoded location entities.\n",
      "Exporting mapping results.\n",
      "Geocoding complete.\n"
     ]
    }
   ],
   "source": [
    "%run autogeocode.py /Users/natalie_kraft/Desktop/city_geocoding_temp.csv gl3 city Recipient \"AidData TUFF Project ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(temp_directory + \"/city_geocoding_temp_results.csv\").rename(columns={'cities_temp_id':'city_temp_id'})\n",
    "geocode_map = geocode_map.append(results[results['city_temp_id'] != \"-1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any result that has been geocoded, restrict from wk \n",
    "temp_wk = wk.merge(results, on='AidData TUFF Project ID', how =\"left\")\n",
    "temp_wk = temp_wk[(pd.isna(temp_wk['city_temp_id'])) | (temp_wk['city_temp_id'] == \"-1\")]\n",
    "wk = temp_wk.drop(columns=['city_temp_id'])\n",
    "\n",
    "assert len(wk) == (len(df) - len(geocode_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are still 8883 more entities to geocode\n"
     ]
    }
   ],
   "source": [
    "print(\"There are still \" + str(len(wk)) + \" more entities to geocode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regional geocoding \n",
    "__Next we can geocode anything that has any regional information present demonstrating a location__\n",
    "- inclusive of standard region naming conventions --> we will consider all of wk['city'] non coded entities to carry over into regional localities as well\n",
    "- and inclusive of \"district of\" and \"province of\" tags \n",
    "- and x \"Region\"/\"Province\"/\"region\"/\"province\"/\"district\"/\"District\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upon inspection, some of the cities actually are administration boundaries. We will run them through the\n",
    "# administrative boundaries API to see if we pick up any localities \n",
    "wk['region'] = \"\"\n",
    "\n",
    "# we will also identify our priority key words to register\n",
    "regional_temp = [re.findall(\"(?:Region|region|Province|province|district|District|state|commune) (?:of|in) (?:[A-Z]\\w*(?:\\s|\\.|,))+\", str(x)) for x in wk['Geographic Location']]\n",
    "regional_temp = [re.sub(\"(Region|region|Province|province|district|District|commune) (of|in) \", \"\", x[0])[:-1] if len(x) !=0 else \"\" for x in regional_temp]\n",
    "\n",
    "regional_temp_2 = [re.findall(\"(?:[A-Z]\\w*(?:\\s|\\.|,))+ (?:Region|region|Province|province|district|District|commune)\", str(x)) for x in wk['Geographic Location']]\n",
    "regional_temp_2 = [re.sub(\" (Region|region|Province|province|district|District)\", \"\", x[0])[:-1] if len(x) !=0 else \"\" for x in regional_temp_2]\n",
    "\n",
    "# consolidate into one columns \n",
    "#wk['region'] = [x if len(x) != 0 else y for x,y in zip(regional_temp, regional_temp_2)]\n",
    "regional_temp = regional_temp + regional_temp_2\n",
    "\n",
    "wk['region'] = [x if len(x) != 0 else y for x,y in zip(regional_temp, wk['city'])]\n",
    "wk['region'] = [re.sub(\"(Province|District|,)\", \"\", x) for x in wk['region']]\n",
    "\n",
    "export = wk[wk['region'] != \"\"][['AidData TUFF Project ID', 'Recipient', 'region']]\n",
    "\n",
    "loc_data = temp_directory + \"/region_geocoding_temp.csv\"\n",
    "export.to_csv(loc_data, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing system configuration.\n",
      "Loading file to geocode\n",
      "You are geocoding regions. Begin geocoding.\n",
      "Loading geocoded location entities.\n",
      "Loading geocoded location entities.\n",
      "\tThis country Africa was not found\n",
      "Location Africa not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Africa was not found\n",
      "Location Africa not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Africa was not found\n",
      "Location Africa not added to mapping or location entities.\n",
      "\tThis country Africa was not found\n",
      "Location Africa not added to mapping or location entities.\n",
      "\tThis country South America was not found\n",
      "Location South America not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Africa was not found\n",
      "Location Africa not added to mapping or location entities.\n",
      "\tThis country South America was not found\n",
      "Location South America not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Africa was not found\n",
      "Location Africa not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Oceania was not found\n",
      "Location Oceania not added to mapping or location entities.\n",
      "\tThis country Multi-Region was not found\n",
      "Location Multi-Region not added to mapping or location entities.\n",
      "\tThis country Multi-Region was not found\n",
      "Location Multi-Region not added to mapping or location entities.\n",
      "\tThis country Africa was not found\n",
      "Location Africa not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Asia was not found\n",
      "Location Asia not added to mapping or location entities.\n",
      "\tThis country Africa was not found\n",
      "Location Africa not added to mapping or location entities.\n",
      "Exporting new regions to database\n",
      "Exporting modified data to geocoded location entities.\n",
      "Exporting mapping results.\n",
      "Geocoding complete.\n"
     ]
    }
   ],
   "source": [
    "%run autogeocode.py /Users/natalie_kraft/Desktop/region_geocoding_temp.csv gl2 region Recipient \"AidData TUFF Project ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(temp_directory + \"/region_geocoding_temp_results.csv\").rename(columns={'regions_temp_id':'region_temp_id'})\n",
    "geocode_map = geocode_map.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any result that has been geocoded, restrict from wk \n",
    "temp_wk = wk.merge(results, on='AidData TUFF Project ID', how =\"left\")\n",
    "temp_wk = temp_wk[(pd.isna(temp_wk['region_temp_id'])) | (temp_wk['region_temp_id'] == \"-1\")]\n",
    "wk = temp_wk.drop(columns=['region_temp_id'])\n",
    "\n",
    "assert len(wk) == (len(df) - len(geocode_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are still 8602 more entities to geocode\n"
     ]
    }
   ],
   "source": [
    "print(\"There are still \" + str(len(wk)) + \" more entities to geocode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country Level Geocoding \n",
    "\n",
    "__We have exhausted all other methods of geocoding__ \n",
    "\n",
    "All remaining locations will be geocoded at the country level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "wk = wk[['AidData TUFF Project ID', 'Recipient', 'Geographic Location']]\n",
    "export = wk\n",
    "\n",
    "loc_data = temp_directory + \"/country_geocoding_temp.csv\"\n",
    "export.to_csv(loc_data, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing system configuration.\n",
      "Loading file to geocode\n",
      "You are geocoding entities at the country level. Begin geocoding.\n",
      "Loading geocoded location entities.\n",
      "Index(['AidData TUFF Project ID', 'Recipient', 'Geographic Location',\n",
      "       'country', 'country_id'],\n",
      "      dtype='object')\n",
      "Exporting mapping results.\n",
      "Geocoding complete.\n"
     ]
    }
   ],
   "source": [
    "%run autogeocode.py /Users/natalie_kraft/Desktop/country_geocoding_temp.csv gl1 Recipient Recipient \"AidData TUFF Project ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(temp_directory + \"/country_geocoding_temp_results.csv\")\n",
    "geocode_map = geocode_map.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are still 168 more entities to geocode\n"
     ]
    }
   ],
   "source": [
    "# any result that has been geocoded, restrict from wk \n",
    "temp_wk = wk.merge(results, on='AidData TUFF Project ID', how =\"left\")\n",
    "wk = temp_wk[(pd.isna(temp_wk['country_id'])) | (temp_wk['country_id'] == \"-1\")]\n",
    "assert len(wk) == (len(df) - len(geocode_map))\n",
    "\n",
    "print(\"There are still \" + str(len(wk)) + \" more entities to geocode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite their being additional entities to geocode, they are defined above the country level. All non-geocoded data will be logged as an unknown point at the coordinates Long: -130.29 Lat: -33.819. \n",
    "\n",
    "## Finalize Geocoded Mapping & Export Geocoded Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the column names to match final conventions\n",
    "geocode_map = geocode_map.rename(columns={'city_temp_id': 'gl3_id', 'region_temp_id': 'gl2_id'})\n",
    "\n",
    "# merge data on TUFF ID \n",
    "merged = df.merge(geocode_map, on='AidData TUFF Project ID', how='left')\n",
    "\n",
    "# export original expenditure data with this map attached\n",
    "merged.to_csv(\"geoaid/geocoded_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/natalie_kraft\n",
      "/Users/natalie_kraft/Desktop\n"
     ]
    }
   ],
   "source": [
    "# we have generated some temporary files. remove them. \n",
    "%cd\n",
    "%cd /Users/natalie_kraft/Desktop\n",
    "\n",
    "%rm city_geocoding_temp.csv city_geocoding_temp_results.csv \n",
    "%rm city_reverse_geocoding_temp.csv city_reverse_geocoding_temp_results.csv\n",
    "\n",
    "%rm country_geocoding_temp.csv country_geocoding_temp_results.csv\n",
    "%rm region_geocoding_temp.csv region_geocoding_temp_results.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
