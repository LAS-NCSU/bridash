{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Public Opinion Data Preprocessing \n",
    "### Pew Public Opinion Data 2017 - 2021\n",
    "\n",
    "The goal of this worksheet is to preprocess the data for easy use moving forward. We hope to: \n",
    "- remove unnessecary data \n",
    "- clean all data so that it has the same labelling conventions (ordinality & missing data point labelling) \n",
    "- combine common data across years for analysis \n",
    "- export individual datasets and a common one across years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology \n",
    "\n",
    "- Import all .sav files as csv with their original labels. This ensures that ordinal variables are transformed in the same direction \n",
    "    - Note: This has typically been produced by using SPSS in the VCL and exporting data with original labels. You also want to grab a dictionary of all the question definitions while you are in there. \n",
    "- Squash down all of the categorical varaibles that are labelled with individual country values \n",
    "- Create a series of dictionaries to transform the values of the rest of the dataset \n",
    "- Drop irrelevant values in each individual dataframe (this will allow a clean dataset to be used for intracountry comparisons) \n",
    "- Determine relevant variables to be used across time (this will allow a clean dataset for intercountry comparisons) \n",
    "    - This dataset would contain individual responses logged with time (year) with the same column values for the same responses. It would NOT be aggregated so that statistics could be filtered by demongraphic, region, etc before comparisons across time. \n",
    "- Export all of this data (1 dataset per year + 1 common dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import ipysheet as ip\n",
    "from ipysheet import sheet, cell, row, column, cell_range, from_dataframe, to_dataframe \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all of the data with original labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2021, 2020, 2019, 2018, 2017]\n",
    "\n",
    "# grabbing the data across the years data is found \n",
    "\n",
    "d = {}\n",
    "\n",
    "path_raw_file = '/Users/natalie_kraft/Documents/LAS/raw/PewL'\n",
    "for year in years: \n",
    "    d[year] = pd.read_csv(path_raw_file + str(year) + '.csv')\n",
    "    d[year].columns = d[year].columns.str.lower()\n",
    "\n",
    "# access dataframe per year through d[year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace Don't Know and Refused with similar verbage \n",
    "# this reduces the number of transformations we need to utilize\n",
    "    \n",
    "for year in years:   \n",
    "    d[year] = d[year].replace({\"(VOL) Don\\'t know\" : \"Don't know\", \n",
    "                 \"Don\\'t know (DO NOT READ)\" : \"Don't know\", \n",
    "                 \"Dont know (DO NOT READ)\" : \"Don't know\",\n",
    "                 \"(VOL)\\xa0Don't know\" : \"Don't know\",\n",
    "                 \"Refused (DO NOT READ)\" : \"Refused\", \n",
    "                 \"(VOL) Refused\" : \"Refused\", \n",
    "                 '(VOL)\\xa0Refused' : \"Refused\", \n",
    "                 ' ': \"Don't know\", \n",
    "                 'Don’t know (DO NOT READ)' : \"Don't know\",\n",
    "                 \"Don\\x92t know (DO NOT READ)\" : \"Don't know\",\n",
    "                 \"Refused (DO NOT READ\" : \"Refused\" })\n",
    "    \n",
    "# attempted this shortcut, but was affected too high proportion of cells \n",
    "#     d[year] = d[year].replace({r\"(.*)Don(.*)know(.*)\" : \"Don't know\", \n",
    "#             r\"(.*)Refused(.*)\" : \"Refused\"}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dOriginal = d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce dimensionality before preprocessing\n",
    "\n",
    "The following variables are individually labelled per country: \n",
    "- 'd_ptyid_' : party identification \n",
    "- 'd_relig_' : religious affiliation \n",
    "- 'd_income2_' : wealthy (rich/poor binary var based on cost of living for country) \n",
    "- 'd_educ_' : level of education respondant recieved\n",
    "\n",
    "The country name already provides this affiliation, dimensionality will be reduced to one common variable for each categorical one. The variable names will be transformed such that: \n",
    "- 'd_ptyid_' : 'political_affiliation'\n",
    "- 'd_relig_' : 'religious_affiliation' \n",
    "- 'd_income2_' : 'wealthy'\n",
    "- 'd_educ_' : 'education_level'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_squash(regex_conven, mapping, new_name, y): \n",
    "    \n",
    "    for year in y: \n",
    "        p_temp = d[year].filter(regex=regex_conven).replace(mapping)\n",
    "        p_temp[new_name] = p_temp.iloc[:, 0]\n",
    "\n",
    "        for index, name in enumerate(p_temp.columns): \n",
    "            p_temp[new_name] = p_temp[new_name].combine_first(p_temp.iloc[:, index])\n",
    "        \n",
    "        d[year][new_name] = p_temp[new_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing political affiliation\n",
    "# keep in mind, this only is a label of affiliation, not a favorability toward individual parties \n",
    "# favorability exists, but it will not be considered in this round of preprocessing \n",
    "\n",
    "categorical_squash('d_ptyid_', {\"Don't know\" : None}, 'political_affiliation', years)\n",
    "\n",
    "# reducing religious affiliation \n",
    "categorical_squash('d_relig_', {\"Don't know\" : None}, 'religious_affiliation', years)\n",
    "\n",
    "# reducing income level\n",
    "# TODO: This needs updating in accordance with 'd_income_'\n",
    "categorical_squash('d_income2_', {\"Don't know\" : None}, 'wealthy', years)\n",
    "# identifing commonalities across respondant's country\n",
    "for year in years: \n",
    "    d[year]['wealthy'].map(lambda x: 1 if (x is not None) and ('More' in x) else 0).value_counts()\n",
    "    \n",
    "categorical_squash('d_educ_', {\"Don't know\" : None}, 'education_level', years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Categorical Squash for Regional Geocoding__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geocoding for 2021, 2020 is region\n",
    "\n",
    "categorical_squash('region_', {\"Don't know\" : None}, 'regional_location', [ 2020])\n",
    "\n",
    "# geocoding for 2019 is region or qs5 \n",
    "\n",
    "categorical_squash('region_', {\"Don't know\" : None}, 'temp_r', [2019])\n",
    "categorical_squash('qs5', {\"Don't know\" : None}, 'temp_q', [2019])\n",
    "d[2019]['regional_location'] = d[2019]['temp_r'].combine_first(d[2019]['temp_q'])\n",
    "d[2019] = d[2019].drop(columns=['temp_r', 'temp_q'])\n",
    "\n",
    "# geocoding for 2018/2017 is qs5\n",
    "\n",
    "categorical_squash('qs5', {\"Don't know\" : None}, 'regional_location', [2018, 2017] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce dimensionality before preprocessing\n",
    "- If the data is a known categorical variable, squash into a common varaible across regions and don't run it through the preprocesser. Or remove categorical variable in its entirity. \n",
    "- If the data is not needed, drop it from the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneeded data from the dataset \n",
    "# This is a listing of variable names per dataset that is unneeded \n",
    "\n",
    "# Absolute column names can be added here\n",
    "drop = {\n",
    "    2021 : ['phone_sample','survey', 'weight', 'qdate_e', 'd_adults', 'diversity_goodbad', 'healthsys_reform', \n",
    "            'basic_facts', 'public', 'polsys_countryfu', 'climate_behavior', 'usdemocracy_example', 'eu_germanyinfluence'], \n",
    "    2020 : ['phone_sample', 'cregion_us', 'density_us', 'covid_change', 'covid_ownfaith', 'covid_countryfaith', 'covid_family',\n",
    "             'covid_united', 'covid_cooperation', 'pray', 'd_adult_us', 'd_political_scale_us', 'd_ptylean_us',\n",
    "            'qs8', 'survey', 'weight', 'qdate_e', 'd_born_us', 'compromise'], \n",
    "    2019 : ['phone_sample', 'cregion_us', 'density_us', 'fav_hezbollah', 'german_unification', 'germany_standard', \n",
    "            'mex_live_us', 'mex_wo_auth', 'survey', 'weight', 'qdate_e', 'd_born_us', 'relparticipate_story', 'fav_muslims_country',  'fav_roma', 'fav_germany', 'receive_money', 'equal_leaders', 'state_us', \n",
    "            'influence_finance', 'fav_muslimbulg', 'neighboring_countries', 'eastwest_ger', \"influence_relig\", \n",
    "           'influence_raise', 'econ_integration', 'country_born', 'fav_jews91', \"kind_of_marriage\", \"same_rights\", \n",
    "           'd_political_scale_us', 'country_national', 'women_rights', 'econ_communism', 'd_political_scale_us', \n",
    "           'close_relationship', 'd_adult_us', 'd_ptylean_us', 'nato_def', 'better_gender', 'us_mil_asia'], \n",
    "    2018 : ['survey', 'weight', 'qdate_e', 'd_born_us', 'state', 'density', 'usr', 'scregion', 'sstate',\n",
    "            'susr','sdensity', 'kashmir_military', 'sanc_effrus', 'mex_live_us', 'workauto50yr', 'good_live_us', \n",
    "           'receive_money', 'immig_moreless'], \n",
    "    2017 : ['survey', 'weight', 'qdate_e', 'd_born_us', 'dem_stable', 'defense_spending', 'desc_day', \n",
    "           'dissol_goodbad', 'eu_leavestay', 'euexit_referendum', 'fav_aap', 'fav_india','fav_pak',\n",
    "            'fav_japan', 'fav_saudi', 'fav_turkey', 'fav_skorea', 'fav_nkorea', 'fav_cuba', 'fav_boko', 'fav_mex',\n",
    "           'fav_eu', 'fav_germany', 'fav_britain', 'fav_nato', 'swe_join_nato', 'turkey_eu_member', 'dissol_goodbad', \n",
    "            'me_role_egypt', 'me_role_saudiarabia', 'me_role_turkey','me_role_iran','me_role_israel',\n",
    "           'fav_sisi','fav_erdogan','fav_assad','fav_netanyahu','fav_salman','fav_rouhani','fav_abdullahii','refugee_iraqsyr',\n",
    "            'war_syria_length', 'fav_adtlpolcnty_rousseff', 'fav_adtlpolcnty_luiz', 'fav_adtlpolcnty_temer',\n",
    "            'fav_lopez', 'fav_radonski', 'fav_allup', 'fav_pri','fav_pan', 'fav_morena', 'fav_prd','fav_modi',\n",
    "            'fav_kejriwal', 'fav_bjp', 'fav_inc','isr_pal_coexist', 'jewish_settlements', 'd_numcell', 'kashmir_military', \n",
    "            'influence_humanrightsorgs', 'nafta_goodbad', 'qsplit',  'racethn', 'racecmb', 'me_role_us', 'prob_kashmir',\n",
    "           'd_density', 'receive_money', 'concern_country', 'humanrights_motive']\n",
    "    \n",
    "}\n",
    "\n",
    "# to reduce names, all partial (or sets) of columns can be added here\n",
    "# if the column name contains any part of this value it will be removed \n",
    "drop_inc_all = ['survey', 'partyfav', \"d_income\", \"d_race\", 'd_ethnicity', \"d_ptyid_\", \"d_educ_\", \"d_relig_\"\n",
    "               'psu_', 'stratum_', 'american_', 'language', 'pray', \"d_working_cell\", \"abortion\", \"covid\", 'ladder', \n",
    "               't.sample', 'homephone', 'confid_johnson', 'confid_macron','confid_merkel', 'confid_castro', 'confid_abe',\n",
    "               'confid_modi', 'd_hhcell', 'fav_eu', 'fav_un', 'fav_iran', 'fav_nato', 'fav_india', 'fav_japan', \n",
    "               'fav_ep', 'fav_ec']\n",
    "\n",
    "drop_inc = {\n",
    "    2021 : ['biden_', 'usbest_', 'conflict_', 'climate_intl', 'discrimprob_'], \n",
    "    2020 : ['sdlkjafsldjflakjsdlfjl'],\n",
    "    2019 : ['multiparty', 'churches_', 'language_home', 'ukr_lang', 'brexit_', 'religion20yr', 'id_', \n",
    "           'equal_'], \n",
    "    2018 : ['qs6', 'qs7', 'qs8', 'qs9', 'qs10', 'qs11', 'cregion', 'robjob4', 'whymove', \n",
    "            'fiveyears_', 'indiaus', 'eu_', 'cyberattack_', '20yr', 'planmove', 'modern_educ', 'friends_', \n",
    "           'officials_', 'pray_', 'relbehavior', 'pairs_'], \n",
    "    2017 : ['brexit_', 'cell_12months', 'church_', 'trump_', 'obama_', 'mfollow_', 'brexit_policy', 'eu_', 'defend_', \n",
    "            'smartphone', 'textfreq', 'turkey_', 'maduro', 'nieto', 'mex_', 'gandhi', 'modi', 'putin_', 'duterte', \n",
    "           'phil', 'italy_pride', 'stayintouch', 'move', 'friends_', 'pray_', 'd_tenure', 'qs6', 'qs7', 'qs8',\n",
    "            'qs9', 'qs10', 'qs11', 'nkorea', 'd_relig_practice'], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data from year 2021 was reduced by 226 columns.\n",
      "The data is now 33\n",
      "The data from year 2020 was reduced by 192 columns.\n",
      "The data is now 39\n",
      "The data from year 2019 was reduced by 521 columns.\n",
      "The data is now 91\n",
      "The data from year 2018 was reduced by 482 columns.\n",
      "The data is now 88\n",
      "The data from year 2017 was reduced by 783 columns.\n",
      "The data is now 97\n"
     ]
    }
   ],
   "source": [
    "for year in years: \n",
    "    sizeInit = d[year].shape[1]\n",
    "    \n",
    "    # ensure that the rest of the data has proper visibility\n",
    "    for i in d[year].columns:\n",
    "        try:\n",
    "            number = d[year][i].value_counts()[\"Don't know\"]\n",
    "            if number > (d[year].shape[0] * .9): \n",
    "                drop[year].append(i)\n",
    "        except KeyError: \n",
    "            # do nothing\n",
    "            number = 0 \n",
    "            \n",
    "    # drop all listed and size-constrained variables \n",
    "    d[year] = d[year].drop(columns=drop[year])\n",
    "    for x in drop_inc[year]: \n",
    "        d[year] = d[year].drop([col for col in d[year].columns if x in col], axis=1)\n",
    "    for x in drop_inc_all: \n",
    "        d[year] = d[year].drop([col for col in d[year].columns if x in col], axis=1)\n",
    "\n",
    "    sizeEnd = d[year].shape[1]\n",
    "    print(\"The data from year \" + str(year) + \" was reduced by \" + str(sizeInit - sizeEnd) + \" columns.\")\n",
    "    print(\"The data is now \" + str(sizeEnd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __We remove all categorical variables in the dataset through dummy variable transformations.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing of all categorical variables to be connected \n",
    "\n",
    "found = {\n",
    "    2021: [],\n",
    "    2020: [], \n",
    "    2019: [], \n",
    "    2018: [], \n",
    "    2017: []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_var(dataset, variable_name, mapping, found): \n",
    "    for year in years: \n",
    "        if variable_name in dataset[year].columns: \n",
    "            print(variable_name + ' variable found in ' + str(year))\n",
    "            dummy_demo = pd.get_dummies(dataset[year][variable_name].map(mapping))\n",
    "            found[year].extend(list(dummy_demo.columns))\n",
    "            \n",
    "            # need to merge dummy variables into df \n",
    "            dataset[year] = pd.concat([dataset[year], dummy_demo], axis=1)\n",
    "            dataset[year] = dataset[year].drop(columns=[variable_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "econ_power variable found in 2020\n",
      "econ_power variable found in 2019\n",
      "econ_power variable found in 2018\n",
      "econ_power variable found in 2017\n"
     ]
    }
   ],
   "source": [
    "# some variables are categorical \n",
    "# they will be transformed into dummy variables and their original label will be removed \n",
    "\n",
    "# econ_power\n",
    "econ_power_mapping = {\n",
    "    \"The United States\": \"US_econ_power\", \n",
    "    \"China\": \"China_econ_power\",\n",
    "    \"Japan\": \"Japan_econ_power\",\n",
    "    \"The countries of the European Union\": \"EU_econ_power\",\n",
    "    \"(VOL) None / There is no leading economic power\": \"no_econ_power\",\n",
    "    \"None / There is no leading economic power (DO NOT READ)\" : \"no_econ_power\",\n",
    "    \"(VOL) Other\": \"other_econ_power\", \n",
    "    \"Other (DO NOT READ)\" : \"other_econ_power\"\n",
    "}\n",
    "\n",
    "# maps onto dummy variables\n",
    "create_dummy_var(d, 'econ_power', econ_power_mapping, found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us_or_china variable found in 2021\n",
      "us_or_china variable found in 2019\n"
     ]
    }
   ],
   "source": [
    "# 'us_or_china'\n",
    "econ_us_china = {\n",
    "    \"The United States\" : \"prefer_us_econ\", \n",
    "    \"China\" : \"prefer_china_econ\", \n",
    "    \"Economic ties to both countries are equally important (DO NOT READ)\" : \"both_china_econ\"\n",
    "}\n",
    "\n",
    "create_dummy_var(d, 'us_or_china', econ_us_china, found)\n",
    "for year in years: \n",
    "    if 'both_china_econ' in d[year].columns: \n",
    "        d[year]['prefer_us_econ'] = d[year]['both_china_econ'] + d[year]['prefer_us_econ']\n",
    "        d[year]['prefer_china_econ'] = d[year]['both_china_econ'] + d[year]['prefer_china_econ']\n",
    "        d[year].drop(columns=['both_china_econ', 'no_econ_power', 'other_econ_power'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worldleader_uschina variable found in 2018\n"
     ]
    }
   ],
   "source": [
    "# econ_power\n",
    "world_leader_mapping = {\n",
    "    \"The U.S. is the world’s leading power\": \"US_better_worldleader\", \n",
    "    \"China is the world’s leading power\": \"China_better_worldleader\",\n",
    "    \"Both (DO NOT READ)\": \"both_better_worldleader\"\n",
    "}\n",
    "\n",
    "# maps onto dummy variables\n",
    "create_dummy_var(d, 'worldleader_uschina', world_leader_mapping, found)\n",
    "for year in years: \n",
    "    if 'both_better_worldleader' in d[year].columns: \n",
    "        d[year]['US_better_worldleader'] = d[year]['both_better_worldleader'] + d[year]['US_better_worldleader']\n",
    "        d[year]['China_better_worldleader'] = d[year]['both_better_worldleader'] + d[year]['China_better_worldleader']\n",
    "        d[year].drop(columns='both_better_worldleader')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify mapping for transformations\n",
    "- Create a list for variables where no transformations are needed \n",
    "- Create all mappings for variables\n",
    "- Search through all variables and map those with similar corresponding labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These column values don't need to be transformed, but we do want to keep them in the dataset \n",
    "# They are either discrete values or they are regional/naming conventions. \n",
    "\n",
    "\n",
    "keep = ['id', 'country', 'sex', 'age', \"d_density\", 'd_hhpeople', 'political_scale2', 'qdate_s',  \"d_density\", \n",
    "        'muslim_branch', 'political_affiliation', 'religious_affiliation', 'education_level', 'wealthy', 'regional_location']\n",
    "\n",
    "keep_inc = {\n",
    "    2021 : [ 'id'], \n",
    "    2020 : [ 'state_us'],\n",
    "    2019 : [ \"allies_new_1\", \"threats_new_1\", 'us_or_china'], \n",
    "    2018 : [ 'qlang', \"d_adult_us\", ], \n",
    "    2017 : [ 'qlang', \"d_adult_us\",], \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vars = {\n",
    "    \"Don't know\": 8, \n",
    "    \"Refused\" : 9\n",
    "}\n",
    "\n",
    "sat_bin = {\n",
    "    \"Dissatisfied\": 0, \n",
    "    \"Satisfied\": 1, \n",
    "}\n",
    "\n",
    "sat_q = {\n",
    "    \"Not too satisfied\": 2, \n",
    "    \"Not at all satisfied\" : 1, \n",
    "    \"Somewhat satisfied\": 3, \n",
    "    \"Very satisfied\": 4, \n",
    "}\n",
    "\n",
    "good_bad_q = {\n",
    "    \"Somewhat good\" : 3,\n",
    "    \"Somewhat bad\" : 2,\n",
    "    \"Very bad\" : 1,\n",
    "    \"Very good\" : 4\n",
    "}\n",
    "\n",
    "better_t = {\n",
    "    \"Worse off\" : 1, \n",
    "    \"Gotten worse\" : 1,\n",
    "    \"Worse\" : 1,\n",
    "    \"Better off\" : 3, \n",
    "    \"Better\" : 3, \n",
    "    \"Gotten better\" : 3, \n",
    "    \"Same (DO NOT READ)\" : 2, \n",
    "    \"Stayed about the same\" : 2, \n",
    "    \"About the same\" : 2\n",
    "}\n",
    "\n",
    "prob_q = {\n",
    "    \"Very big problem\" : 4, \n",
    "    \"Moderately big problem\" : 3, \n",
    "    \"Small problem\" : 2, \n",
    "    \"Not a problem at all\" : 1, \n",
    "}\n",
    "\n",
    "fav_q = {\n",
    "    \"Somewhat favorable\" : 3, \n",
    "    \"Somewhat unfavorable\" : 2, \n",
    "    \"Very favorable\": 4, \n",
    "    \"Very unfavorable\" : 1\n",
    "}\n",
    "\n",
    "amount_q = {\n",
    "    \"Great deal\" : 4, \n",
    "    \"A great deal\" : 4,\n",
    "    \"Fair amount\" : 3, \n",
    "    \"A fair amount\" : 3,\n",
    "    \"Not too much\" : 2, \n",
    "    \"Not at all\" : 1\n",
    "}\n",
    "\n",
    "approval_q = {\n",
    "    \"Approve\": 3, \n",
    "    \"Strongly approve\": 4, \n",
    "    \"Disapprove\" : 2, \n",
    "    \"Strongly disapprove\" : 1\n",
    "} \n",
    "\n",
    "confid_q = {\n",
    "    \"No confidence at all\" : 1, \n",
    "    \"Not too much confidence\" : 2, \n",
    "    \"Some confidence\" : 3, \n",
    "    \"A lot of confidence\" : 4\n",
    "}\n",
    "\n",
    "right_t = {\n",
    "    \"About right\" : 2, \n",
    "    \"Too great\" : 3, \n",
    "    \"Too small\" : 1\n",
    "}\n",
    "\n",
    "yesno_bin = {\n",
    "    \"No\" : 0, \n",
    "    \"Yes\" : 1\n",
    "}\n",
    "\n",
    "influe_q = {\n",
    "    \"Great deal of influence\" : 4,\n",
    "    \"Very good influence\" : 4, \n",
    "    \"Fair amount of influence\" : 3, \n",
    "    \"Good influence\" : 3, \n",
    "    \"Bad influence\" : 2, \n",
    "    \"Very bad influence\" : 1, \n",
    "    \"Not too much influence\" : 2, \n",
    "    \"No influence at all\" : 1, \n",
    "    \"No influence (DO NOT READ)\" : 8\n",
    "}\n",
    "  \n",
    "mil_bin = {\n",
    "    \"Yes, would use military force\" : 1, \n",
    "    \"Yes, should use military force\" : 1, \n",
    "    \"No, would not use military force\" : 0, \n",
    "    \"No, should not use military force\" : 0\n",
    "}\n",
    "\n",
    "import_q = {\n",
    "    \"Very important\" : 4,  \n",
    "    \"Somewhat important\" : 3,\n",
    "    \"Not very important\" : 2,\n",
    "    \"Not too important\" :2, \n",
    "    \"Not at all important\": 1, \n",
    "    \"Not important at all\": 1  \n",
    "}\n",
    "\n",
    "roles_t = {\n",
    "    \"More important role\" : 3, \n",
    "    \"Doing more\" : 3, \n",
    "    \"Less important role\" : 1, \n",
    "    \"Doing less\" : 1, \n",
    "    \"As important as 10 years ago\" : 2, \n",
    "    \"U.S. does not help (DO NOT READ)\" : 2, \n",
    "    \"About the same\" : 2\n",
    "}\n",
    "\n",
    "threat_t = {\n",
    "    \"Major threat\" : 3, \n",
    "    \"Not a threat\" : 1, \n",
    "    \"Minor threat\" : 2, \n",
    "    'Very concerned' : 3,\n",
    "    'Somewhat concerned' : 2, \n",
    "    'Not too concerned' : 2, \n",
    "    'Not at all concerned' : 1\n",
    "}\n",
    "\n",
    "changes_t = {\n",
    "    'It needs to be completely reformed' : 4, \n",
    "    'It needs major changes' : 3, \n",
    "    'It needs minor changes' : 2, \n",
    "    'It doesn’t need to be changed' : 1\n",
    "}\n",
    "\n",
    "god_bin = {\n",
    "    \"It is necessary to believe in God in order to be moral and have good values\" : 1, \n",
    "    \"It is not necessary to believe in God in order to be moral and have good values\" : 0\n",
    "}\n",
    "\n",
    "china_bin = {\n",
    "    \"The U.S. should try to promote human rights in China, even if it harms economic relations with China\" : 1,\n",
    "    '(response in COUNTRY) should try to promote human rights in China, even if it harms econo' : 1, \n",
    "    \"The U.S. should prioritize strengthening economic relations with China, even if it means not addressing human rights iss\" : 0,\n",
    "    '(response in COUNTRY) should prioritize strengthening economic relations with China, even if it means not addressing' : 0\n",
    "}\n",
    "\n",
    "priority_q = {\n",
    "    \"Top priority\" : 4, \n",
    "    \"Important but lower priority\" : 3, \n",
    "    \"Not too important\" : 2, \n",
    "    \"Should not work on this issue\" : 1\n",
    "}\n",
    "\n",
    "trust_bin = {\n",
    "    \"In general, most people can be trusted\" : 1,\n",
    "    \"In general, most people cannot be trusted\" :0\n",
    "}\n",
    "\n",
    "econ_q = {\n",
    "    \"Improve a lot\" : 4, \n",
    "    \"Improve a little\" : 3,\n",
    "    \"Worsen a little\" : 2,\n",
    "    \"Worsen a lot\" :1, \n",
    "    \"Remain the same\" : 2.5\n",
    "}\n",
    "\n",
    "trust_q = {\n",
    "    \"A lot\" : 4, \n",
    "    \"Somewhat\" : 3, \n",
    "    \"Not much\" : 2, \n",
    "    \"Not at all\" : 1\n",
    "}\n",
    "\n",
    "enemy_t = {\n",
    "    \"Competitor\" : 2,     \n",
    "    \"Enemy\" :3,\n",
    "    \"Partner\" :1\n",
    "}\n",
    "\n",
    "agree_q = {\n",
    "    \"Mostly disagree\" : 2, \n",
    "    \"Completely disagree\" :1, \n",
    "    \"Mostly agree\" : 3, \n",
    "    \"Completely agree\" : 4, \n",
    "}\n",
    "\n",
    "goodbad_b = {\n",
    "    \"Bad thing\" : 0, \n",
    "    \"Good thing\" : 1, \n",
    "    \"Neither good nor bad\" : 0, \n",
    "    \"Neither (DO NOT READ)\" : 0,\n",
    "    \"Both (DO NOT READ)\" :0\n",
    "}\n",
    "\n",
    "goodbad_b2 = {\n",
    "    \"Investment from China is a good thing\" : 1, \n",
    "    \"Investment from China is a bad thing\" : 0\n",
    "}\n",
    "\n",
    "posneg_b  = {\n",
    "    \"Positive\" : 1, \n",
    "    \"Negative\" : 0, \n",
    "    \"Neither/both (DO NOT READ)\" : 0   \n",
    "}\n",
    "\n",
    "opto_b = {\n",
    "    \"Optimistic\" : 1, \n",
    "    \"Pessimistic\" : 0, \n",
    "    \"Neither (DO NOT READ)\" : 0\n",
    "}\n",
    "\n",
    "smart_b = {\n",
    "    \"Smartphone\" : 1,\n",
    "    \"Not a smartphone\" : 0\n",
    "}\n",
    "\n",
    "cell_b = {\n",
    "    \"Yes, someone in household has cell phone\" : 1, \n",
    "    \"No\" : 0 \n",
    "}\n",
    "\n",
    "global_b = {\n",
    "    \"should act as part of a global community that works together to solve problems\" : 1,                      \n",
    "    \"should act as independent nations that compete with other countries and pursue their own interests\" : 0, \n",
    "    \"Both (DO NOT READ)\" : 1, \n",
    "    \"Neither (DO NOT READ)\" : 0                  \n",
    "}\n",
    "\n",
    "homo_b = {\n",
    "    \"Homosexuality should be accepted by society\" : 1,\n",
    "    \"Homosexuality should not be accepted by society\" : 0,  \n",
    "}\n",
    "\n",
    "relat_b = {\n",
    "    \"Building a strong relationship with China on economic issues\" : 0,\n",
    "    \"Getting tougher with China on economic issues\" : 1\n",
    "}\n",
    "\n",
    "news_q = {\n",
    "    \"Very well\" : 4, \n",
    "    \"Somewhat well\" :3, \n",
    "    \"Not too well\" : 2, \n",
    "    \"Not well at all\" : 1, \n",
    "    \"News organizations should not do this (DO NOT READ)\" : 0\n",
    "}\n",
    "\n",
    "news_b = {\n",
    "    \"It is never acceptable for a news organization to favor one political party over others when reporting news\" : 0, \n",
    "    \"It is sometimes acceptable for a news organization to favor one political party over others when reporting news\" : 1\n",
    "}\n",
    "\n",
    "respect_b = {\n",
    "    \"Yes, respects personal freedoms\" : 1, \n",
    "    \"No, does not respect personal freedoms\" : 0 \n",
    "}\n",
    "\n",
    "support_b = {\n",
    "    \"Support\" : 1, \n",
    "    \"Oppose\" : 0\n",
    "}\n",
    "    \n",
    "civic_q = {\n",
    "    \"Have done in the past year\" : 4, \n",
    "    \"Have done in the more distant past\" : 3, \n",
    "    \"Have not done, but might do\" : 2, \n",
    "    \"Have not done and would never, under any circumstances, do\" : 1\n",
    "}\n",
    "\n",
    "increase_t = {\n",
    "    \"Increase\" : 3, \n",
    "    \"Decrease\" : 1,\n",
    "    \"Does not make a difference\" : 2\n",
    "}\n",
    "\n",
    "nukes_t = {\n",
    "    \"Too much\" : 3, \n",
    "    \"About what needs to be done OR\" : 2, \n",
    "    \"Too little\" : 1\n",
    "}\n",
    "\n",
    "jobs_t = {\n",
    "    \"Job creation\" : 3, \n",
    "    \"Job losses\" : 1, \n",
    "    \"Does not make a difference\" : 2\n",
    "}\n",
    "\n",
    "likely_q = {\n",
    "    \"Very likely\" : 4, \n",
    "    \"Somewhat likely\" : 3, \n",
    "    \"Not too likely\" : 2, \n",
    "    \"Not at all likely\" : 1\n",
    "}\n",
    "\n",
    "social_s = {\n",
    "    \"Several times a day\" : 7,  \n",
    "    \"Once a day\" : 6,       \n",
    "    \"Several times a week\" : 5, \n",
    "    \"Once a week\" : 4, \n",
    "    \"Several times a month\" : 3, \n",
    "    \"Once a month\" : 2, \n",
    "    \"Less than once a month\" : 1,  \n",
    "    \"Never\" : 0,                    \n",
    "}\n",
    "\n",
    "better_place_t = {\n",
    "    \"A better place to live\" : 3, \n",
    "    \"A worse place to live\" : 1, \n",
    "    \"Doesn\\'t make much difference either way (DO NOT READ)\" : 2\n",
    "}\n",
    "\n",
    "reliability = {\n",
    "    \"Very reliable\" : 4,\n",
    "    \"Somewhat reliable\" : 3, \n",
    "    \"Not too reliable\" : 2,\n",
    "    \"Not at all reliable\" : 1, \n",
    "}\n",
    "\n",
    "dictionaries = [sat_bin, good_bad_q, sat_q, better_t, fav_q, amount_q, approval_q, confid_q, right_t, \n",
    "               yesno_bin, influe_q, mil_bin, import_q, roles_t, threat_t, prob_q, trust_q]\n",
    "\n",
    "dictionaries_niche = [god_bin, trust_bin, china_bin, econ_q, enemy_t, agree_q, goodbad_b, goodbad_b2,  \n",
    "                      posneg_b, opto_b, smart_b, global_b, homo_b, relat_b, news_q, respect_b, support_b, \n",
    "                     civic_q, increase_t, nukes_t, jobs_t, likely_q, social_s, priority_q, news_b, \n",
    "                     cell_b, better_place_t, reliability, changes_t]\n",
    "\n",
    "# ensures the missing variables are included in the datasets \n",
    "\n",
    "for i in dictionaries: \n",
    "    i.update(missing_vars)\n",
    "    \n",
    "for i in dictionaries_niche: \n",
    "    i.update(missing_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function provides a matching mechanism for data labels into a numeric scale \n",
    "# This scale is constant across years (where positive responses are ranked highest)\n",
    "# The original dataset is overrridden with these transformations \n",
    "def preprocess(year, dictionaries, found):\n",
    "    \n",
    "    for i in d[year].columns: \n",
    "        if ('qs' not in i) and ('region' not in i) and (i not in keep) and (i not in keep_inc[year]) and (i not in found[year]): \n",
    "            for di in dictionaries: \n",
    "                if len(set(d[year][i]).difference(set(di.keys()))) == 0: \n",
    "                    found[year].append(i)\n",
    "                    d[year][i] = d[year][i].map(di)\n",
    "                    break\n",
    "                       \n",
    "            for di in dictionaries_niche: \n",
    "                if len(set(d[year][i]).difference(set(di.keys()))) == 0: \n",
    "                    found[year].append(i)\n",
    "                    d[year][i] = d[year][i].map(di)\n",
    "                    break\n",
    "                       \n",
    "        else: \n",
    "            found[year].append(i)\n",
    "                \n",
    "    return d[year], found[year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE 2021 columns \"reliable_us\" --> \"relations_us\" and \"climate_concern\" --> \"intthreat_climatechange\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently, we are preprocessing year 2021\n",
      "There were 36 columns preprocessed.\n",
      "This means that there were 0 columns left to support: \n",
      "set()\n",
      "\n",
      "--------------------------------------------\n",
      "Currently, we are preprocessing year 2020\n",
      "There were 48 columns preprocessed.\n",
      "This means that there were 0 columns left to support: \n",
      "set()\n",
      "\n",
      "--------------------------------------------\n",
      "Currently, we are preprocessing year 2019\n",
      "There were 107 columns preprocessed.\n",
      "This means that there were 0 columns left to support: \n",
      "set()\n",
      "\n",
      "--------------------------------------------\n",
      "Currently, we are preprocessing year 2018\n",
      "There were 100 columns preprocessed.\n",
      "This means that there were 0 columns left to support: \n",
      "set()\n",
      "\n",
      "--------------------------------------------\n",
      "Currently, we are preprocessing year 2017\n",
      "There were 108 columns preprocessed.\n",
      "This means that there were 0 columns left to support: \n",
      "set()\n",
      "\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for year in years: \n",
    "    \n",
    "    print(\"Currently, we are preprocessing year \" + str(year))\n",
    "    d[year], found[year] = preprocess(year, dictionaries, found)\n",
    "    print(\"There were \" + str(len(found[year])) + \" columns preprocessed.\")\n",
    "    print(\"This means that there were \" + str(len(set(d[year].columns).difference(set(found[year])))) + \" columns left to support: \")\n",
    "    print(set(d[year].columns).difference(set(found[year])))\n",
    "    print(\"\")\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Merge - reducing dimensionality \n",
    "\n",
    "- each year has a time frame added when the survey was conducted \n",
    "- data is merged on like column names \n",
    "- dimensionality reduction showcased "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the survey year to the dataframe \n",
    "for year in years: \n",
    "    d[year]['surveyYear'] = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensionality of this year is 16254 by 35\n",
      "The dimensionality of this year is 30530 by 79\n",
      "The dimensionality of this year is 68956 by 178\n",
      "The dimensionality of this year is 99065 by 272\n",
      "The dimensionality of this year is 141018 by 375\n"
     ]
    }
   ],
   "source": [
    "# assessing data dimensionality \n",
    "count = 0\n",
    "col = 0\n",
    "for year in years: \n",
    "    count = d[year].shape[0] + count\n",
    "    col = d[year].shape[1] + col\n",
    "    print(\"The dimensionality of this year is \" + str(count) + \" by \" + str(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensionality of the combined data is 141018 by 244\n",
      "In total, we have captured 100.0% of the data after the merge.\n",
      "There has been a 53.69% decrease in column through overlapping.\n"
     ]
    }
   ],
   "source": [
    "# data merge \n",
    "\n",
    "df = pd.DataFrame()\n",
    "for year in years: \n",
    "    df = df.append(d[year].reset_index())\n",
    "    \n",
    "print(\"The dimensionality of the combined data is \" + str(df.shape[0]) + \" by \" + str(df.shape[1]))\n",
    "print(\"In total, we have captured \" + str(round(count * 100 / df.shape[0], 2)) + \"% of the data after the merge.\")\n",
    "print(\"There has been a \" + str(round((col - df.shape[1]) * 100 / df.shape[1], 2)) + \"% decrease in column through overlapping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Across all of our years of data 10 columns are present across the dataset. \n"
     ]
    }
   ],
   "source": [
    "print(\"Across all of our years of data \" + str(df.dropna(axis=1).shape[1]) + \" columns are present across the dataset. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Commonalities \n",
    "\n",
    "Through individual parsing of data, it is possible that variables denoting the same question with different variable names have been listed as seperate columns in the merged dataset. We look to identify any variables of identical questions that were NOT merged accordingly and manually adjust the final spreadsheet. \n",
    "\n",
    "Currently, we have a secondary sheet logged within each original file which contains the actual questioned asked of respondants. Our goal is to identify similarity between these questions, we then can confirm these mappings and then transform the overlapping variables to the same name. Here is the methodology: \n",
    "\n",
    "- Create a mapping between the question variable name and the question itself for each variable in every year. \n",
    "- Use WordMoverDistance to identify semantic similarities \n",
    "- Provide a listing of variables with the questions listed for approval \n",
    "- Verified pairs will be listed in a dataframe \n",
    "- All verified pairs will be replaced with a common variable name \n",
    "\n",
    "Then we can return to merging our data together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__View a listing of all of the variables for each year and how they match across years with__ \n",
    "\n",
    "``` \n",
    "varAll\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all common variable names to a separate dataframe \n",
    "# columns - years & final_var_name\n",
    "# rows - variable names \n",
    "# every row will be given a final_var_name which will replace the var_name for that year before the merge \n",
    "\n",
    "# create a dataframe showing which variables map to which final_variable_names \n",
    "# first instantiate it with the variables which do match across df \n",
    "\n",
    "varAll = {\n",
    "    2020 : pd.DataFrame(d[2020].columns, columns=['2020']).reset_index(drop=True), \n",
    "    2019 : pd.DataFrame(d[2019].columns, columns=['2019']).reset_index(drop=True), \n",
    "    2018 : pd.DataFrame(d[2018].columns, columns=['2018']).reset_index(drop=True), \n",
    "    2017 : pd.DataFrame(d[2017].columns, columns=['2017']).reset_index(drop=True)\n",
    "}\n",
    "\n",
    "var = varAll[2020]\n",
    "var['temp'] = var['2020']\n",
    "var = var[[\"temp\", \"2020\"]]\n",
    "\n",
    "\n",
    "# for every year \n",
    "for year in years[1:]: \n",
    "    # merge in the next year \n",
    "    var = var.merge(varAll[year], left_on = 'temp', right_on = str(year), how='outer')\n",
    "    var = var.fillna('nan')\n",
    "    # add in the final_var_name to ensure that all variables are being matched \n",
    "    var['temp'] = var['temp'].combine(var[str(year)], func = (lambda x1, x2: x1 if x1 is not 'nan' else x2))\n",
    "    \n",
    "varOriginal = var "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making an interactive table for ease of mapping variables \n",
    "# This was deemed computationally too expensive, instead the table is exported for editting and will be reimported \n",
    "# when complete \n",
    "# sheet = from_dataframe(var)\n",
    "# sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find the questions for each variable name through searching__\n",
    "\n",
    "```\n",
    "qMap[year][variable_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the question labels in \n",
    "\n",
    "# remove the 'Q#' label at the beginning \n",
    "\n",
    "# pair the question labels with the variable names \n",
    "# we are building a dictionary (accessible by year : [(label, name)] pairs) \n",
    "\n",
    "qMap = {\n",
    "    2020 : {}, \n",
    "    2019 : {}, \n",
    "    2018 : {}, \n",
    "    2017 : {}\n",
    "}\n",
    "\n",
    "for year in years: \n",
    "    tempdf = pd.read_excel(\"/Users/natalie_kraft/Documents/LAS/bridash/data_processing/pew/pewQVDict.xlsx\", sheet_name = str(year))\n",
    "    # drop any variables which are not used by the main source code, there is no need to further process these \n",
    "    tempdf['variable_name'] = tempdf['variable_name'].str.lower()\n",
    "    tempdf[tempdf['variable_name'].isin(list(d[year].columns)) == True].shape\n",
    "    qMap[year] = dict(zip(tempdf['variable_name'], tempdf['question']))\n",
    "    \n",
    "    # remove all of the 'Q'\n",
    "    for key in qMap[year]: \n",
    "        if qMap[year][key][:1] == 'Q': \n",
    "            qMap[year][key] = qMap[year][key].split(\".\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all columns from key:value pair that are no longer in dataset \n",
    "for year in years: \n",
    "    pop_variables = list(set(qMap[year].keys()).difference(set(varAll[year][str(year)])))\n",
    "    [qMap[year].pop(x, None) for x in pop_variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Identify semantic similarities in question labels__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilize Word2Vec to identify semantic similarities in question labels \n",
    "# print out similar questions and variable names for clarification \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: identify where we will collect the \"correct\" pairings for reference and to reduce the list. \n",
    "# If we can identify a threshold, that could work to automatically add them to the final df \n",
    "# *** We can manually add values by establishing an interactive table in the notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranformations\n",
    "\n",
    "For consistency, conduct your final transformations prior to export. This includes mapping all religious affiliations to categories. \n",
    "\n",
    "Note: consider bridging over political affiliations as well. Requires lots of research on political groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation for all religious beliefs. \n",
    "# If the label CONTAINS the key, it should be grouped into the value categories. \n",
    "relig_transform  = {\n",
    "    r\"(.*)Christian(.*)\" : \"Christian\", \n",
    "    r\"(.*)Unitarian(.*)\" : \"Christian\", \n",
    "    r\"(.*)Agnostic(.*)\" : \"Agnostic\", \n",
    "    r\"(.*)African(.*)\" : \"traditional African religion\", \n",
    "    r\"(.*)Atheist(.*)\" : \"Atheist\", \n",
    "    r\"(.*)Baha(.*)\" : \"Bahai\", \n",
    "    r\"(.*)Buddhis(.*)\" : \"Buddhist\",\n",
    "    r\"(.*)Buddist(.*)\" : \"Buddhist\", \n",
    "    r\"(.*)Catholic(.*)\" : \"Catholic\", \n",
    "    r\"(.*)Confucianism(.*)\" : \"Confucianism\", \n",
    "    r\"(.*)Congregationalist(.*)\" : \"Protestant\", \n",
    "    r\"(.*)Druze(.*)\" : \"Druze\", \n",
    "    r\"(.*)Evangelical(.*)\" : \"Protestant\", \n",
    "    r\"(.*)Hindu(.*)\" : \"Hindu\", \n",
    "    r\"(.*)Iglesia ni Cristo(.*)\" : \"Christian\", \n",
    "    r\"(.*)Indigenous religion(.*)\" : \"Indigenous religion\", \n",
    "    r\"(.*)Jain(.*)\" : \"Jain\", \n",
    "    r\"(.*)Jehova(.*)\" : \"Restorationist Christian\",  \n",
    "    r\"(.*)Jew(.*)\" : \"Jewish\", \n",
    "    r\"(.*)Lutheran(.*)\" : \"Protestant\", \n",
    "    r\"(.*)Mormon(.*)\" : \"Restorationist Christian\", \n",
    "    r\"(.*)Muslim(.*)\" : \"Muslim\", \n",
    "    r\"(.*)No(.*)\" : None, \n",
    "    r\"(.*)Orthodox(.*)\" : \"Catholic\", \n",
    "    r\"(.*)Pentecostal(.*)\" : \"Protestant\", \n",
    "    r\"(.*)Presbyterian(.*)\" : \"Protestant\", \n",
    "    r\"(.*)Protestant(.*)\" : \"Protestant\", \n",
    "    r\"(.*)Sikh(.*)\" : \"Sikh\", \n",
    "    r\"(.*)Something else(.*)\" : \"religious\",\n",
    "    r\"(.*)Spiritist(.*)\" : \"Spiritist\", \n",
    "    r\"(.*)Refused(.*)\" : None,\n",
    "    r\"(.*)Afrobrazilian religion(.*)\" : \"Afrobrazilian religion\", \n",
    "    r\"(.*)Unification(.*)\" : \"Christian\", \n",
    "    r\"(.*)Unitarian(.*)\" : \"Christian\", \n",
    "    r\"(.*)Yes(.*)\" : \"religious\", \n",
    "}\n",
    "\n",
    "df['religious_affiliation'] = df['religious_affiliation'].replace(regex=relig_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix \n",
    "\n",
    "Throughout the preprocessing of this data, several areas of expansion were identified. These include: \n",
    "- __parsing of political affililation__ - data currently includes favorability to 'mainstream' parties and party affiliation. Transformation to leaning across countries could be valuable. Currently, all political identification moved toward one generic variable \"political_party\". \n",
    "- __updating income level__ - there is a variable 'd_income2_' that supposedly categorizes wealth. This label is inaccurate and doesn't log all of respondants wealth, regardless of a variable 'd_income_' that has this granularity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-299d5495bd3f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-299d5495bd3f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    { \"id\":\"Dalian\", \"title\":\"Dalian Container Terminal Co., Ltd.\", \"share\": \"19\", \"target\": \"18\", \"TEU\": \"9,500,000\", \"depth\": \"17.8\", \"url\": \"http://www.dct.com.cn/\"}, { \"id\":\"Dalian\", \"title\":\"Dalian Dagang China Shipping Container Terminal Co., Ltd.\", \"share\": \"35\", \"target\": \"1\", \"TEU\": \"100,000\", \"depth\": \"9.1\", \"url\": \"\"}, { \"id\":\"Dalian\", \"title\":\"Dalian Automobile Terminal Co., Ltd.\", \"share\": \"24\", \"target_v\": \"3\", \"TEU_v\": \"780,000\", \"depth\": \"11.0\", \"url\": \"\"}, { \"id\":\"Tianjin\", \"title\":\"Tianjin Port Container Terminal Co., Ltd.\", \"share\": \"51\", \"target\": \"13\", \"TEU\": \"6,000,000\", \"depth\": \"12.0-17.0\", \"url\": \"http://www.tianjin-port.com/port/Show.asp?m=1&d=1096\"}, { \"id\":\"Yingkou\", \"title\":\"Yingkou Container Terminals Company Limited\", \"share\": \"50\", \"target\": \"2\", \"TEU\": \" 1,200,000 \", \"depth\": \"14.0\", \"url\": \"http://221.202.72.96:9090/\"}, { \"id\":\"Yingkou\", \"title\":\"Yingkou New Century Container Terminal Co., Ltd.\", \"share\": \"40\", \"target\": \"2\", \"TEU\": \"1,200,000\", \"depth\": \"15.5\", \"url\": \"\"}, { \"id\":\"Jinzhou\", \"title\":\"Jinzhou New Age Container Terminal Co., Ltd.\", \"share\": \"51\", \"target\": \"2\", \"TEU\": \"800,000\", \"depth\": \"15.4\", \"url\": \"http://www.jnct.com.cn/\"}, { \"id\":\"Qinhuangdao\", \"title\":\"Qinhuangdao Port New Harbour Container Terminal Co., Ltd.\", \"share\": \"30\", \"target\": \"2\", \"TEU\": \"950,000\", \"depth\": \"15.8\", \"url\": \"http://bid.coal.com.cn/Member/Company_316070.htm\"}, { \"id\":\"Qingdao\", \"title\":\"Qingdao Port International Co., Ltd.\", \"share\": \"19.79\", \"target\": \"24\", \"TEU\": \"10,000,000\", \"target2\": \"62\", \"TEU2\": \"207,020,000\", \"depth\": \"\", \"url\": \"http://www.qingdao-port.com\"}, { \"id\":\"Qingdao\", \"title\":\"Qingdao Port Dongjiakou Ore Terminal Co., Ltd.\", \"share2\": \"25\", \"target2\": \"3\", \"TEU2\": \"29,000,000\", \"depth2\": \"20.0-25.0\", \"url\": \"\"}, { \"id\":\"Shanghai\", \"title\":\"Shanghai Pudong International Container Terminals Limited\", \"share\": \"30\", \"target\": \"3\", \"TEU\": \"2,300,000\", \"depth\": \"12.0\", \"url\": \"http://www.spict.com/\"}, { \"id\":\"Shanghai\", \"title\":\"Shanghai Mingdong Container Terminals Limited\", \"share\": \"20\", \"target\": \"7\", \"TEU\": \"5,600,000\", \"depth\": \"12.8\", \"url\": \"https://www.smct.com.cn\"}, { \"id\":\"Ningbo\", \"title\":\"Ningbo Yuan Dong Terminals Limited\", \"share\": \"20\", \"target\": \"3\", \"TEU\": \"3,000,000\", \"depth\": \"17.1\", \"url\": \"\"}, { \"id\":\"Ningbo\", \"title\":\"Ningbo Meishan Bonded Port New Harbour Terminal Operating Co., Ltd.\", \"share\": \"20\", \"target\": \"2\", \"TEU\": \"1,200,000\", \"depth\": \"15.6\", \"url\": \"\"}, { \"id\":\"Lianyungang\", \"title\":\"Lianyungang New Oriental International Terminals Co., Ltd.\", \"share\": \"55\", \"target\": \"4\", \"TEU\": \"1,400,000\", \"depth\": \"11.5-15.0\", \"url\": \"http://www.lnoct.com\"}, { \"id\":\"Taicang\", \"title\":\"Taicang International Container Terminal Co., Ltd.\", \"share\": \"39.04\", \"target\": \"2\", \"TEU\": \"550,000\", \"depth\": \"12.0\", \"target2\": \"2\", \"TEU2\": \"4,000,000\", \"depth2\": \"12.0\", \"url\": \"http://www.taicangterminals.com/\"}, { \"id\":\"Wuhan\", \"title\":\"Wushan CSP Terminal Co., Ltd.\", \"share\": \"70\", \"target\": \"4\", \"TEU\": \"77,200\", \"depth\": \"6.4\", \"url\": \"\", \"target2\": \"4\", \"TEU2\": \"4,200,000\",\"depth2\": \"6.4\"}, { \"id\":\"Nantong\", \"title\":\"Nantong Tonghai Port Co., Ltd.\", \"share\": \"51\", \"target\": \"3\", \"TEU\": \"1,470,000\", \"depth\": \"9.0-11.0\", \"target2\": \"1\", \"TEU2\": \"5,370,000\", \"depth2\": \"6.0\"/*, \"remark\":\"\"*/}, { \"id\":\"Xiamen\", \"title\":\"Xiamen Ocean Gate Container Terminal Co., Ltd.\", \"share\": \"70\", \"target\": \"4\", \"TEU\": \"2,600,000\", \"depth\": \"15.0\", \"target2\": \"1\", \"TEU2\": \"4,000,000\", \"depth2\": \"6.6-13.6\", \"url\": \"http://www.coscoyh.com.cn/\"}, { \"id\":\"Quanzhou\", \"title\":\"Quan Zhou Pacific Container Terminal Co., Ltd.\", \"share\": \"82.35\", \"target\": \"5\", \"TEU\": \"3,000,000\", \"depth\": \"11.6-15.1\", \"target2\": \"2\", \"TEU2\": \"1,000,000\", \"depth2\": \"5.1-9.6\", \"url\": \"http://www.qpct.com.cn/\"}, { \"id\":\"Jinjiang\", \"title\":\"Jinjiang Pacific Ports Development Co., Ltd.\", \"share\": \"80\", \"target\": \"2\", \"TEU\": \"600,000\", \"depth\": \"9.5-15.3\", \"target2\": \"2\", \"TEU2\": \"4,200,000\", \"depth2\": \"7.5-9.5\", \"url\": \"http://jppdc.com.cn/lxweb/index.asp\"}, { \"id\":\"Kaohsiung\", \"title\":\"Kao Ming Container Terminal Corp.\", \"share\": \"20\", \"target\": \"4\", \"TEU\": \"2,800,000\", \"depth\": \"16.5\", \"url\": \"http://www.kmct.com.tw/index.php\"}, { \"id\":\"Shenzhen\", \"title\":\"Yantian International Container Terminals Co., Ltd.\", \"share\": \"14.59\"/*, \"target\": \"5\", \"TEU\": \"4,500,000\", \"depth\": \"14.0-15.5\", \"url\": \"\"*/, \"bf1\":\"<b>Phase 1, 2</b>\"}, { \"id\":\"Shenzhen\", \"title\":\"\", \"share\": \"13.36\", /*\"target\": \"20\", \"TEU\": \"13,000,000\", \"depth\": \"14.0-17.6\", \"url\": \"https://www.yict.com.cn/index.html?locale=zh_CN\",*/ \"bf1\":\"<b>Phase 3</b>\"}, { \"id\":\"Shenzhen\", \"title\":\"\", \"target\": \"20\", \"TEU\": \"13,000,000\", \"depth\": \"14.0-17.6\", /*\"url\": \"https://www.yict.com.cn/index.html?locale=zh_CN\",*/ \"bf1\":\"<b>Phase 1, 2, 3</b>\"}, { \"id\":\"Guangzhou\", \"title\":\"Guangzhou South China Oceangate Container Terminal Company Limited\", \"share\": \"39\", \"target\": \"6\", \"TEU\": \"4,200,000\", \"depth\": \"15.5\", \"url\": \"https://www.goct.com.cn/\"}, { \"id\":\"Guangzhou\", \"title\":\"Nansha Stevedoring Corporation Limited of Port of Guangzhou\", \"share\": \"40\", \"target\": \"4\", \"TEU\": \"5,000,000\", \"depth\": \"14.5-15.5\",\"url\":\"http://www.gnict.com/\"}, { \"id\":\"Hong\", \"title\":\"COSCO-HIT Terminals (Hong Kong) Limited\", \"share\": \"50\", \"target\": \"2\", \"TEU\": \"1,800,000\", \"depth\": \"15.5\", \"url\": \"https://hutchisonports.com/ports/world/cosco-hit-terminals-hong-kong-limited-cht/\"}, { \"id\":\"Hong\", \"title\":\"Asia Container Terminals Limited\", \"share\": \"60\", \"target\": \"2\", \"TEU\": \"1,600,000\", \"depth\": \"15.5\", \"url\": \"https://hutchisonports.com/ports/world/asia-container-terminals-limited-act/\"}, { \"id\":\"Qinzhou\", \"title\":\"Guangxi Beibu Gulf International Container Terminal Co., Ltd.\", \"share\": \"30.69\", \"target\": \"6\", \"TEU\": \"3,600,000\", \"depth\": \"15.1\"}, { \"id\":\"Qinzhou\", \"title\":\"Beibu Gulf Port Co., Ltd.<sup>Note</sup>\", \"share\": \"10.65\", \"target\": \"18.0\", \"TEU\": \"10,800,000\", \"target2\": \"100\", \"TEU2\": \"268,400,000\", \"depth\": \"\", \"remark\":\"Note: The target no. of berths and the target designed annual handing capacity do not include Beibu Gulf Terminal.\", \"url\": \"https://www.bbwport.cn\"}, { \"id\":\"Piraeus\", \"title\":\"Piraeus Container Terminal Single Member S.A.\", \"share\": \"100\", \"target\": \"8\", \"TEU\": \"6,200,000\", \"depth\": \"14.5-19.5\", \"url\": \"http://www.pct.com.gr/\"}, { \"id\":\"Zeebrugge\", \"title\":\"CSP Zeebrugge Terminals NV\", \"share\": \"90\", \"target\": \"3\", \"TEU\": \"1,300,000\", \"depth\": \"17.5\", \"url\": \"http://www.cspterminals.be/\"}, { \"id\":\"Noatum\", \"title\":\"CSP Iberian Valencia Terminal, S.A.U.\", \"share\": \"51\", \"target\": \"6\", \"TEU\": \"4,100,000\", \"depth\": \"16\", \"url\": \"https://www.cspspain.com/EN\"}, { \"id\":\"Noatum\", \"title\":\"CSP Iberian Bilbao Terminal, S.L.\", \"share\": \"39.51\", \"target\": \"3\", \"TEU\": \"1,000,000\", \"depth\": \"21\"}, { \"id\":\"COSCO\", \"title\":\"COSCO-PSA Terminal Private Limited\", \"share\": \"49\", \"target\": \"5\", \"TEU\": \"4,850,000\", \"depth\": \"18.0\", \"url\": \"https://www.singaporepsa.com/our-business/terminals/joint-venture-terminals\"}, { \"id\":\"Vado\", \"title\":\"Reefer Terminal S.p.A.\", \"share\": \"40\", \"target\": \"2\", \"TEU\": \"250,000\", \"depth\": \"14.5\", \"target2_p\": \"2\", \"TEU2_p\": \"600,000\", \"depth2\": \"14.1\"}, { \"id\":\"Vado\", \"title\":\"Vado Container Terminal\", \"share\": \"40\", \"target\": \"2\", \"TEU\": \"860,000\", \"depth\": \"17.25\", \"url\": \"http://www.apmterminals.com/en/operations/europe/vado-ligure\"}, { \"id\":\"Kumport\", \"title\":\"Kumport Liman Hizmetleri ve Lojistik Sanayi ve Ticaret <nobr>A. Ş.</nobr>\", \"share\": \"26\", \"target\": \"6\", \"TEU\": \"2,100,000\", \"depth\": \"15.0-16.5\", \"url\": \"http://www.kumport.com.tr/en-US\"}, { \"id\":\"Suez\", \"title\":\"Suez Canal Container Terminal S.A.E.\", \"share\": \"20\", \"target\": \"8\", \"TEU\": \"5,000,000\", \"depth\": \"17.0\", \"url\": \"https://scct.com.eg/\"}, { \"id\":\"Euromax\", \"title\":\"Euromax Terminal Rotterdam B.V.\", \"share\": \"17.85\", \"target\": \"5\", \"TEU\": \"3,200,000\", \"depth\": \"17.65\"}, { \"id\":\"Antwerp\", \"title\":\"Antwerp Gateway NV\", \"share\": \"20\", \"target\": \"4\", \"TEU\": \"3,700,000\", \"depth\": \"16.0\", \"url\": \"http://www.dpworldantwerp.com/our-businesses/antwerp-gateway\"}, { \"id\":\"Seattle\", \"title\":\"SSA Terminals (Seattle), LLC\", \"share\": \"13.33\", \"target\": \"2\", \"TEU\": \"400,000\", \"depth\": \"15.2\", \"url\": \"http://www.ssamarine.com/locations/terminal-30/\"}, { \"id\":\"Busan\", \"title\":\"Busan Port Terminal Co., Ltd.\", \"share\": \"4.89\", \"target\": \"8\", \"TEU\": \"4,000,000\", \"depth\": \"15.0-16.0\"}, { \"id\":\"Abu\", \"title\":\"CSP Abu Dhabi Terminal L.L.C.\", \"share\": \"40\", \"target\": \"3\", \"TEU\": \"2,500,000 (Phase 2)\", \"depth\": \"18\"/*, \"remark\":\"Expected operation commencement date: third quarter in 2019\"*/}, { \"id\":\"Red\", \"title\":\"Red Sea Gateway Terminal\", \"share\": \"20\", \"target\": \"11\", \"TEU\": \"5,200,000\", \"depth\": \"18.0\"}, { \"id\":\"Chancay\", \"title\":\"COSCO SHIPPING Ports Chancay PERU S.A.<sup>*</sup>\", \"share\": \"60\", \"target\": \"2\", \"TEU\": \"1,000,000\", \"depth\": \"16-18\", \"target2\": \"2\", \"TEU2\": \"6,200,000\", \"depth2\": \"14\", \"remark\":\"* Under Construction\"} ];\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "{ \"id\":\"Dalian\", \"title\":\"Dalian Container Terminal Co., Ltd.\", \"share\": \"19\", \"target\": \"18\", \"TEU\": \"9,500,000\", \"depth\": \"17.8\", \"url\": \"http://www.dct.com.cn/\"}, { \"id\":\"Dalian\", \"title\":\"Dalian Dagang China Shipping Container Terminal Co., Ltd.\", \"share\": \"35\", \"target\": \"1\", \"TEU\": \"100,000\", \"depth\": \"9.1\", \"url\": \"\"}, { \"id\":\"Dalian\", \"title\":\"Dalian Automobile Terminal Co., Ltd.\", \"share\": \"24\", \"target_v\": \"3\", \"TEU_v\": \"780,000\", \"depth\": \"11.0\", \"url\": \"\"}, { \"id\":\"Tianjin\", \"title\":\"Tianjin Port Container Terminal Co., Ltd.\", \"share\": \"51\", \"target\": \"13\", \"TEU\": \"6,000,000\", \"depth\": \"12.0-17.0\", \"url\": \"http://www.tianjin-port.com/port/Show.asp?m=1&d=1096\"}, { \"id\":\"Yingkou\", \"title\":\"Yingkou Container Terminals Company Limited\", \"share\": \"50\", \"target\": \"2\", \"TEU\": \" 1,200,000 \", \"depth\": \"14.0\", \"url\": \"http://221.202.72.96:9090/\"}, { \"id\":\"Yingkou\", \"title\":\"Yingkou New Century Container Terminal Co., Ltd.\", \"share\": \"40\", \"target\": \"2\", \"TEU\": \"1,200,000\", \"depth\": \"15.5\", \"url\": \"\"}, { \"id\":\"Jinzhou\", \"title\":\"Jinzhou New Age Container Terminal Co., Ltd.\", \"share\": \"51\", \"target\": \"2\", \"TEU\": \"800,000\", \"depth\": \"15.4\", \"url\": \"http://www.jnct.com.cn/\"}, { \"id\":\"Qinhuangdao\", \"title\":\"Qinhuangdao Port New Harbour Container Terminal Co., Ltd.\", \"share\": \"30\", \"target\": \"2\", \"TEU\": \"950,000\", \"depth\": \"15.8\", \"url\": \"http://bid.coal.com.cn/Member/Company_316070.htm\"}, { \"id\":\"Qingdao\", \"title\":\"Qingdao Port International Co., Ltd.\", \"share\": \"19.79\", \"target\": \"24\", \"TEU\": \"10,000,000\", \"target2\": \"62\", \"TEU2\": \"207,020,000\", \"depth\": \"\", \"url\": \"http://www.qingdao-port.com\"}, { \"id\":\"Qingdao\", \"title\":\"Qingdao Port Dongjiakou Ore Terminal Co., Ltd.\", \"share2\": \"25\", \"target2\": \"3\", \"TEU2\": \"29,000,000\", \"depth2\": \"20.0-25.0\", \"url\": \"\"}, { \"id\":\"Shanghai\", \"title\":\"Shanghai Pudong International Container Terminals Limited\", \"share\": \"30\", \"target\": \"3\", \"TEU\": \"2,300,000\", \"depth\": \"12.0\", \"url\": \"http://www.spict.com/\"}, { \"id\":\"Shanghai\", \"title\":\"Shanghai Mingdong Container Terminals Limited\", \"share\": \"20\", \"target\": \"7\", \"TEU\": \"5,600,000\", \"depth\": \"12.8\", \"url\": \"https://www.smct.com.cn\"}, { \"id\":\"Ningbo\", \"title\":\"Ningbo Yuan Dong Terminals Limited\", \"share\": \"20\", \"target\": \"3\", \"TEU\": \"3,000,000\", \"depth\": \"17.1\", \"url\": \"\"}, { \"id\":\"Ningbo\", \"title\":\"Ningbo Meishan Bonded Port New Harbour Terminal Operating Co., Ltd.\", \"share\": \"20\", \"target\": \"2\", \"TEU\": \"1,200,000\", \"depth\": \"15.6\", \"url\": \"\"}, { \"id\":\"Lianyungang\", \"title\":\"Lianyungang New Oriental International Terminals Co., Ltd.\", \"share\": \"55\", \"target\": \"4\", \"TEU\": \"1,400,000\", \"depth\": \"11.5-15.0\", \"url\": \"http://www.lnoct.com\"}, { \"id\":\"Taicang\", \"title\":\"Taicang International Container Terminal Co., Ltd.\", \"share\": \"39.04\", \"target\": \"2\", \"TEU\": \"550,000\", \"depth\": \"12.0\", \"target2\": \"2\", \"TEU2\": \"4,000,000\", \"depth2\": \"12.0\", \"url\": \"http://www.taicangterminals.com/\"}, { \"id\":\"Wuhan\", \"title\":\"Wushan CSP Terminal Co., Ltd.\", \"share\": \"70\", \"target\": \"4\", \"TEU\": \"77,200\", \"depth\": \"6.4\", \"url\": \"\", \"target2\": \"4\", \"TEU2\": \"4,200,000\",\"depth2\": \"6.4\"}, { \"id\":\"Nantong\", \"title\":\"Nantong Tonghai Port Co., Ltd.\", \"share\": \"51\", \"target\": \"3\", \"TEU\": \"1,470,000\", \"depth\": \"9.0-11.0\", \"target2\": \"1\", \"TEU2\": \"5,370,000\", \"depth2\": \"6.0\"/*, \"remark\":\"\"*/}, { \"id\":\"Xiamen\", \"title\":\"Xiamen Ocean Gate Container Terminal Co., Ltd.\", \"share\": \"70\", \"target\": \"4\", \"TEU\": \"2,600,000\", \"depth\": \"15.0\", \"target2\": \"1\", \"TEU2\": \"4,000,000\", \"depth2\": \"6.6-13.6\", \"url\": \"http://www.coscoyh.com.cn/\"}, { \"id\":\"Quanzhou\", \"title\":\"Quan Zhou Pacific Container Terminal Co., Ltd.\", \"share\": \"82.35\", \"target\": \"5\", \"TEU\": \"3,000,000\", \"depth\": \"11.6-15.1\", \"target2\": \"2\", \"TEU2\": \"1,000,000\", \"depth2\": \"5.1-9.6\", \"url\": \"http://www.qpct.com.cn/\"}, { \"id\":\"Jinjiang\", \"title\":\"Jinjiang Pacific Ports Development Co., Ltd.\", \"share\": \"80\", \"target\": \"2\", \"TEU\": \"600,000\", \"depth\": \"9.5-15.3\", \"target2\": \"2\", \"TEU2\": \"4,200,000\", \"depth2\": \"7.5-9.5\", \"url\": \"http://jppdc.com.cn/lxweb/index.asp\"}, { \"id\":\"Kaohsiung\", \"title\":\"Kao Ming Container Terminal Corp.\", \"share\": \"20\", \"target\": \"4\", \"TEU\": \"2,800,000\", \"depth\": \"16.5\", \"url\": \"http://www.kmct.com.tw/index.php\"}, { \"id\":\"Shenzhen\", \"title\":\"Yantian International Container Terminals Co., Ltd.\", \"share\": \"14.59\"/*, \"target\": \"5\", \"TEU\": \"4,500,000\", \"depth\": \"14.0-15.5\", \"url\": \"\"*/, \"bf1\":\"<b>Phase 1, 2</b>\"}, { \"id\":\"Shenzhen\", \"title\":\"\", \"share\": \"13.36\", /*\"target\": \"20\", \"TEU\": \"13,000,000\", \"depth\": \"14.0-17.6\", \"url\": \"https://www.yict.com.cn/index.html?locale=zh_CN\",*/ \"bf1\":\"<b>Phase 3</b>\"}, { \"id\":\"Shenzhen\", \"title\":\"\", \"target\": \"20\", \"TEU\": \"13,000,000\", \"depth\": \"14.0-17.6\", /*\"url\": \"https://www.yict.com.cn/index.html?locale=zh_CN\",*/ \"bf1\":\"<b>Phase 1, 2, 3</b>\"}, { \"id\":\"Guangzhou\", \"title\":\"Guangzhou South China Oceangate Container Terminal Company Limited\", \"share\": \"39\", \"target\": \"6\", \"TEU\": \"4,200,000\", \"depth\": \"15.5\", \"url\": \"https://www.goct.com.cn/\"}, { \"id\":\"Guangzhou\", \"title\":\"Nansha Stevedoring Corporation Limited of Port of Guangzhou\", \"share\": \"40\", \"target\": \"4\", \"TEU\": \"5,000,000\", \"depth\": \"14.5-15.5\",\"url\":\"http://www.gnict.com/\"}, { \"id\":\"Hong\", \"title\":\"COSCO-HIT Terminals (Hong Kong) Limited\", \"share\": \"50\", \"target\": \"2\", \"TEU\": \"1,800,000\", \"depth\": \"15.5\", \"url\": \"https://hutchisonports.com/ports/world/cosco-hit-terminals-hong-kong-limited-cht/\"}, { \"id\":\"Hong\", \"title\":\"Asia Container Terminals Limited\", \"share\": \"60\", \"target\": \"2\", \"TEU\": \"1,600,000\", \"depth\": \"15.5\", \"url\": \"https://hutchisonports.com/ports/world/asia-container-terminals-limited-act/\"}, { \"id\":\"Qinzhou\", \"title\":\"Guangxi Beibu Gulf International Container Terminal Co., Ltd.\", \"share\": \"30.69\", \"target\": \"6\", \"TEU\": \"3,600,000\", \"depth\": \"15.1\"}, { \"id\":\"Qinzhou\", \"title\":\"Beibu Gulf Port Co., Ltd.<sup>Note</sup>\", \"share\": \"10.65\", \"target\": \"18.0\", \"TEU\": \"10,800,000\", \"target2\": \"100\", \"TEU2\": \"268,400,000\", \"depth\": \"\", \"remark\":\"Note: The target no. of berths and the target designed annual handing capacity do not include Beibu Gulf Terminal.\", \"url\": \"https://www.bbwport.cn\"}, { \"id\":\"Piraeus\", \"title\":\"Piraeus Container Terminal Single Member S.A.\", \"share\": \"100\", \"target\": \"8\", \"TEU\": \"6,200,000\", \"depth\": \"14.5-19.5\", \"url\": \"http://www.pct.com.gr/\"}, { \"id\":\"Zeebrugge\", \"title\":\"CSP Zeebrugge Terminals NV\", \"share\": \"90\", \"target\": \"3\", \"TEU\": \"1,300,000\", \"depth\": \"17.5\", \"url\": \"http://www.cspterminals.be/\"}, { \"id\":\"Noatum\", \"title\":\"CSP Iberian Valencia Terminal, S.A.U.\", \"share\": \"51\", \"target\": \"6\", \"TEU\": \"4,100,000\", \"depth\": \"16\", \"url\": \"https://www.cspspain.com/EN\"}, { \"id\":\"Noatum\", \"title\":\"CSP Iberian Bilbao Terminal, S.L.\", \"share\": \"39.51\", \"target\": \"3\", \"TEU\": \"1,000,000\", \"depth\": \"21\"}, { \"id\":\"COSCO\", \"title\":\"COSCO-PSA Terminal Private Limited\", \"share\": \"49\", \"target\": \"5\", \"TEU\": \"4,850,000\", \"depth\": \"18.0\", \"url\": \"https://www.singaporepsa.com/our-business/terminals/joint-venture-terminals\"}, { \"id\":\"Vado\", \"title\":\"Reefer Terminal S.p.A.\", \"share\": \"40\", \"target\": \"2\", \"TEU\": \"250,000\", \"depth\": \"14.5\", \"target2_p\": \"2\", \"TEU2_p\": \"600,000\", \"depth2\": \"14.1\"}, { \"id\":\"Vado\", \"title\":\"Vado Container Terminal\", \"share\": \"40\", \"target\": \"2\", \"TEU\": \"860,000\", \"depth\": \"17.25\", \"url\": \"http://www.apmterminals.com/en/operations/europe/vado-ligure\"}, { \"id\":\"Kumport\", \"title\":\"Kumport Liman Hizmetleri ve Lojistik Sanayi ve Ticaret <nobr>A. Ş.</nobr>\", \"share\": \"26\", \"target\": \"6\", \"TEU\": \"2,100,000\", \"depth\": \"15.0-16.5\", \"url\": \"http://www.kumport.com.tr/en-US\"}, { \"id\":\"Suez\", \"title\":\"Suez Canal Container Terminal S.A.E.\", \"share\": \"20\", \"target\": \"8\", \"TEU\": \"5,000,000\", \"depth\": \"17.0\", \"url\": \"https://scct.com.eg/\"}, { \"id\":\"Euromax\", \"title\":\"Euromax Terminal Rotterdam B.V.\", \"share\": \"17.85\", \"target\": \"5\", \"TEU\": \"3,200,000\", \"depth\": \"17.65\"}, { \"id\":\"Antwerp\", \"title\":\"Antwerp Gateway NV\", \"share\": \"20\", \"target\": \"4\", \"TEU\": \"3,700,000\", \"depth\": \"16.0\", \"url\": \"http://www.dpworldantwerp.com/our-businesses/antwerp-gateway\"}, { \"id\":\"Seattle\", \"title\":\"SSA Terminals (Seattle), LLC\", \"share\": \"13.33\", \"target\": \"2\", \"TEU\": \"400,000\", \"depth\": \"15.2\", \"url\": \"http://www.ssamarine.com/locations/terminal-30/\"}, { \"id\":\"Busan\", \"title\":\"Busan Port Terminal Co., Ltd.\", \"share\": \"4.89\", \"target\": \"8\", \"TEU\": \"4,000,000\", \"depth\": \"15.0-16.0\"}, { \"id\":\"Abu\", \"title\":\"CSP Abu Dhabi Terminal L.L.C.\", \"share\": \"40\", \"target\": \"3\", \"TEU\": \"2,500,000 (Phase 2)\", \"depth\": \"18\"/*, \"remark\":\"Expected operation commencement date: third quarter in 2019\"*/}, { \"id\":\"Red\", \"title\":\"Red Sea Gateway Terminal\", \"share\": \"20\", \"target\": \"11\", \"TEU\": \"5,200,000\", \"depth\": \"18.0\"}, { \"id\":\"Chancay\", \"title\":\"COSCO SHIPPING Ports Chancay PERU S.A.<sup>*</sup>\", \"share\": \"60\", \"target\": \"2\", \"TEU\": \"1,000,000\", \"depth\": \"16-18\", \"target2\": \"2\", \"TEU2\": \"6,200,000\", \"depth2\": \"14\", \"remark\":\"* Under Construction\"} ]; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
