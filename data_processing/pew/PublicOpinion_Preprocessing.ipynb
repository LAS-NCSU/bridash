{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Public Opinion Data Preprocessing \n",
    "### Pew Public Opinion Data 2017 - 2021\n",
    "\n",
    "The goal of this worksheet is to preprocess the data for easy use moving forward. We hope to: \n",
    "- remove unnessecary data \n",
    "- clean all data so that it has the same labelling conventions (ordinality & missing data point labelling) \n",
    "- combine common data across years for analysis \n",
    "- export individual datasets and a common one across years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology \n",
    "\n",
    "- Import all .sav files as csv with their original labels. This ensures that ordinal variables are transformed in the same direction \n",
    "    - Note: This has typically been produced by using SPSS in the VCL and exporting data with original labels. You also want to grab a dictionary of all the question definitions while you are in there. \n",
    "- Squash down all of the categorical varaibles that are labelled with individual country values \n",
    "- Create a series of dictionaries to transform the values of the rest of the dataset \n",
    "- Drop irrelevant values in each individual dataframe (this will allow a clean dataset to be used for intracountry comparisons) \n",
    "- Determine relevant variables to be used across time (this will allow a clean dataset for intercountry comparisons) \n",
    "    - This dataset would contain individual responses logged with time (year) with the same column values for the same responses. It would NOT be aggregated so that statistics could be filtered by demongraphic, region, etc before comparisons across time. \n",
    "- Export all of this data (1 dataset per year + 1 common dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all of the data with original labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/natalie_kraft/Documents/LAS/raw/PewL2021.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m path_raw_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/natalie_kraft/Documents/LAS/raw/PewL\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m years: \n\u001b[0;32m----> 9\u001b[0m     d[year] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_raw_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     d[year]\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m d[year]\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/natalie_kraft/Documents/LAS/raw/PewL2021.csv'"
     ]
    }
   ],
   "source": [
    "years = [2021, 2020, 2019, 2018, 2017]\n",
    "\n",
    "# grabbing the data across the years data is found \n",
    "\n",
    "d = {}\n",
    "\n",
    "path_raw_file = '/Users/natalie_kraft/Documents/LAS/raw/PewL'\n",
    "for year in years: \n",
    "    d[year] = pd.read_csv(path_raw_file + str(year) + '.csv')\n",
    "    d[year].columns = d[year].columns.str.lower()\n",
    "\n",
    "# access dataframe per year through d[year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace Don't Know and Refused with similar verbage \n",
    "# this reduces the number of transformations we need to utilize\n",
    "    \n",
    "for year in years:   \n",
    "    d[year] = d[year].replace({\"(VOL) Don\\'t know\" : \"Don't know\", \n",
    "                 \"Don\\'t know (DO NOT READ)\" : \"Don't know\", \n",
    "                 \"Dont know (DO NOT READ)\" : \"Don't know\",\n",
    "                 \"(VOL)\\xa0Don't know\" : \"Don't know\",\n",
    "                 \"Refused (DO NOT READ)\" : \"Refused\", \n",
    "                 \"(VOL) Refused\" : \"Refused\", \n",
    "                 '(VOL)\\xa0Refused' : \"Refused\", \n",
    "                 ' ': \"Don't know\", \n",
    "                 'Don’t know (DO NOT READ)' : \"Don't know\",\n",
    "                 \"Don\\x92t know (DO NOT READ)\" : \"Don't know\",\n",
    "                 \"Refused (DO NOT READ\" : \"Refused\", \n",
    "                 'Dont know' : \"Don't know\"})\n",
    "    \n",
    "# attempted this shortcut, but was affected too high proportion of cells \n",
    "#     d[year] = d[year].replace({r\"(.*)Don(.*)know(.*)\" : \"Don't know\", \n",
    "#             r\"(.*)Refused(.*)\" : \"Refused\"}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dOriginal = d.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some variables are the same, but need slight recoding. Do that here. \n",
    "# this block focuses on 'intthreat_uspower' and 'intthreat_chpower'\n",
    "\n",
    "dOriginal[2019]['us_influ_econ2'].replace({ \"Positive\" : \"Not a threat\", \n",
    " \"Negative\" : \"Threat\", \n",
    " \"Neither/both (DO NOT READ)\" : \"Not a threat\"\n",
    "}, inplace = True) \n",
    "\n",
    "dOriginal[2019].rename(columns={'us_influ_econ2': 'intthreat_uspower'})\n",
    "\n",
    "dOriginal[2018]['intthreat_uspower'].replace({\"Major threat\" : \"Threat\", \"Minor threat\": \"Threat\"}, inplace=True)\n",
    "dOriginal[2017]['intthreat_uspower'].replace({\"Major threat\" : \"Threat\", \"Minor threat\": \"Threat\"}, inplace=True)\n",
    "\n",
    "dOriginal[2019]['china_influ_econ2'].replace({ \"Positive\" : \"Not a threat\", \n",
    " \"Negative\" : \"Threat\", \n",
    " \"Neither/both (DO NOT READ)\" : \"Not a threat\"\n",
    "}, inplace = True) \n",
    "\n",
    "dOriginal[2019].rename(columns={'china_influ_econ2': 'intthreat_chpower'})\n",
    "\n",
    "dOriginal[2018]['intthreat_chpower'].replace({\"Major threat\" : \"Threat\", \"Minor threat\": \"Threat\"}, inplace=True)\n",
    "dOriginal[2017]['intthreat_chpower'].replace({\"Major threat\" : \"Threat\", \"Minor threat\": \"Threat\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus on us/china influence scales. Combining variables for consistency. \n",
    "\n",
    "infl_map = {\"Fair amount of influence\" : \"Tries to influence the internal affairs of other countries\", \n",
    "                                             \"Great deal of influence\" : \"Tries to influence the internal affairs of other countries\", \n",
    "                                             \"Not too much influence\" : \"Mostly stays out of the internal affairs of other countries\" , \n",
    "                                             \"No influence at all\" : \"Mostly stays out of the internal affairs of other countries\"}\n",
    "\n",
    "dOriginal[2019]['china_influ_econ'].replace(infl_map, inplace=True)\n",
    "dOriginal[2019]['us_influ_econ'].replace(infl_map, inplace=True)\n",
    "dOriginal[2019].rename(columns={'china_influ_econ': 'influaffairs_china', 'us_influ_econ' : 'influaffairs_us'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dOriginal[2021].rename(columns={'polsys_country': 'polsys_reform'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating a variable to showcase the relationship countries have with the US\n",
    "\n",
    "dOriginal[2021]['reliable_us'].replace({'Somewhat reliable': 'Somewhat good', 'Not too reliable' : 'Somewhat bad', \n",
    "                                       'Very reliable' : 'Very good', 'Not at all reliable' : 'Very bad' }, inplace=True)\n",
    "\n",
    "dOriginal[2021].rename(columns={'reliable_us': 'us_relationship'}, inplace=True)\n",
    "dOriginal[2019].rename(columns={'relations_us': 'us_relationship'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one variable - econ_ties_china - is in 2 years with different meanings. \n",
    "\n",
    "dOriginal[2020].rename(columns={'econ_ties_china' : 'econ_ties_usch'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for categorical variables to be kept throughout dataset for legibility in mapping \n",
    "\n",
    "categorical_vars = ['children_betteroff2', 'fav_us', 'fav_china', 'fav_russia', 'country_satis', 'satisfied_democracy', \n",
    "                   'confid_biden', 'confid_trump', 'confid_xi', 'confid_putin', 'econ_sit', 'econ_power', 'us_or_china', \n",
    "                   'religion_import', 'respect_china', 'respect_us', 'intthreat_uspower', 'intthreat_chpower', 'improve_econ', \n",
    "                   'intthreat_econcondition', 'us_relationship', 'china_econ', 'china_military']\n",
    "\n",
    "only_categorical_vars = ['global_trade', 'global_movement', 'global_information', 'china_jobloss', 'china_deficit', \n",
    "                         'china_taiwan', 'china_environ', 'china_debt', 'china_terrdisputes', 'us_def_china', \n",
    "                         'china_econ_military', 'us_world_role', 'world_role_china', 'world_role_russia', \n",
    "                         'usrel_betterworse', 'involved_us', 'influaffairs_russia', 'interest_surveycountry', \n",
    "                         'econ_ties_china', 'econ_ties_us', 'threats_new_1', 'allies_new_1', 'foreigncom_buy', \n",
    "                         'foreigncom_new', 'china_invest', 'future_usrel', 'close_relationship', 'us_mil_asia', \n",
    "                         'china_us_enemy', 'econsys_reform', 'china_tough', 'china_tough_econ', 'intthreat_ruspower', \n",
    "                        'influaffairs_china', 'influaffairs_us', 'polsys_reform', 'econ_ties_usch']\n",
    "\n",
    "for year in years: \n",
    "    for c in categorical_vars: \n",
    "        if c in d[year].columns: \n",
    "            d[year][c + \"_categorical\"] = d[year][c]\n",
    "    for c in only_categorical_vars: \n",
    "        if c in d[year].columns: \n",
    "            d[year][c + \"_categorical\"] = d[year][c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce dimensionality before preprocessing\n",
    "\n",
    "The following variables are individually labelled per country: \n",
    "- 'd_ptyid_' : party identification \n",
    "- 'd_relig_' : religious affiliation \n",
    "- 'd_income2_' : wealthy (rich/poor binary var based on cost of living for country) \n",
    "- 'd_educ_' : level of education respondant recieved\n",
    "\n",
    "The country name already provides this affiliation, dimensionality will be reduced to one common variable for each categorical one. The variable names will be transformed such that: \n",
    "- 'd_ptyid_' : 'political_affiliation'\n",
    "- 'd_relig_' : 'religious_affiliation' \n",
    "- 'd_income2_' : 'wealthy'\n",
    "- 'd_educ_' : 'education_level'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_squash(regex_conven, mapping, new_name, y): \n",
    "    \n",
    "    for year in y: \n",
    "        p_temp = d[year].filter(regex=regex_conven).replace(mapping)\n",
    "        p_temp[new_name] = p_temp.iloc[:, 0]\n",
    "\n",
    "        for index, name in enumerate(p_temp.columns): \n",
    "            p_temp[new_name] = p_temp[new_name].combine_first(p_temp.iloc[:, index])\n",
    "        \n",
    "        d[year][new_name] = p_temp[new_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing political affiliation\n",
    "# keep in mind, this only is a label of affiliation, not a favorability toward individual parties \n",
    "# favorability exists, but it will not be considered in this round of preprocessing \n",
    "\n",
    "categorical_squash('d_ptyid_', {\"Don't know\" : None}, 'political_affiliation', years)\n",
    "\n",
    "# reducing religious affiliation \n",
    "categorical_squash('d_relig_', {\"Don't know\" : None}, 'religious_affiliation', years)\n",
    "\n",
    "# reducing income level\n",
    "# TODO: This needs updating in accordance with 'd_income_'\n",
    "categorical_squash('d_income2_', {\"Don't know\" : None}, 'wealthy', years)\n",
    "# identifing commonalities across respondant's country\n",
    "for year in years: \n",
    "    d[year]['wealthy'].map(lambda x: 1 if (x is not None) and ('More' in x) else 0).value_counts()\n",
    "    \n",
    "categorical_squash('d_educ_', {\"Don't know\" : None}, 'education_level', years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Categorical Squash for Regional Geocoding__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geocoding for 2021, 2020 is region\n",
    "\n",
    "categorical_squash('region_', {\"Don't know\" : None}, 'regional_location', [2020, 2021])\n",
    "\n",
    "# geocoding for 2019 is region or qs5 \n",
    "\n",
    "categorical_squash('region_', {\"Don't know\" : None}, 'temp_r', [2019])\n",
    "categorical_squash('qs5', {\"Don't know\" : None}, 'temp_q', [2019])\n",
    "d[2019]['regional_location'] = d[2019]['temp_r'].combine_first(d[2019]['temp_q'])\n",
    "d[2019] = d[2019].drop(columns=['temp_r', 'temp_q'])\n",
    "\n",
    "# geocoding for 2018/2017 is qs5\n",
    "\n",
    "categorical_squash('qs5', {\"Don't know\" : None}, 'regional_location', [2018, 2017] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce dimensionality before preprocessing\n",
    "- If the data is a known categorical variable, squash into a common varaible across regions and don't run it through the preprocesser. Or remove categorical variable in its entirity. \n",
    "- If the data is not needed, drop it from the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneeded data from the dataset \n",
    "# This is a listing of variable names per dataset that is unneeded \n",
    "\n",
    "# Absolute column names can be added here\n",
    "drop = {\n",
    "    2021 : ['phone_sample','survey', 'weight', 'diversity_goodbad', 'healthsys_reform', \n",
    "            'basic_facts', 'public', 'polsys_countryfu', 'climate_behavior', 'usdemocracy_example', 'eu_germanyinfluence', \n",
    "            'biden_relations'], \n",
    "    2020 : ['phone_sample', 'cregion_us', 'density_us', 'covid_change', 'covid_ownfaith', 'covid_countryfaith', 'covid_family',\n",
    "             'covid_united', 'covid_cooperation', 'pray', 'd_political_scale_us', 'd_ptylean_us',\n",
    "            'qs8', 'survey', 'weight', 'd_born_us', 'compromise'], \n",
    "    2019 : ['phone_sample', 'cregion_us', 'density_us', 'fav_hezbollah', 'german_unification', 'germany_standard', \n",
    "            'mex_live_us', 'mex_wo_auth', 'survey', 'weight', 'd_born_us', 'relparticipate_story', 'fav_muslims_country',  \n",
    "            'fav_roma', 'fav_germany', 'receive_money', 'equal_leaders', 'state_us', \n",
    "            'influence_finance', 'fav_muslimbulg', 'neighboring_countries', 'eastwest_ger', \"influence_relig\", \n",
    "           'influence_raise', 'econ_integration', 'country_born', 'fav_jews91', \"kind_of_marriage\", \"same_rights\", \n",
    "           'd_political_scale_us', 'country_national', 'women_rights', 'econ_communism', 'd_political_scale_us', \n",
    "           'close_relationship', 'd_ptylean_us', 'nato_def', 'better_gender', 'us_mil_asia', \n",
    "           'confid_orban', 'confid_kim', 'confid_salman', 'id_religion', 'id_nationality', 'id_occupation', \n",
    "            'id_polparty'], \n",
    "    2018 : ['survey', 'weight', 'd_born_us', 'state', 'density', 'usr', 'scregion', 'sstate',\n",
    "            'susr','sdensity', 'kashmir_military', 'sanc_effrus', 'mex_live_us', 'workauto50yr', 'good_live_us', \n",
    "           'receive_money', 'immig_moreless'], \n",
    "    2017 : ['survey', 'weight', 'd_born_us', 'dem_stable', 'defense_spending', 'desc_day', \n",
    "           'dissol_goodbad', 'eu_leavestay', 'euexit_referendum', 'fav_aap', 'fav_india','fav_pak',\n",
    "            'fav_japan', 'fav_saudi', 'fav_turkey', 'fav_skorea', 'fav_nkorea', 'fav_cuba', 'fav_boko', 'fav_mex',\n",
    "           'fav_eu', 'fav_germany', 'fav_britain', 'fav_nato', 'swe_join_nato', 'turkey_eu_member', 'dissol_goodbad', \n",
    "            'me_role_egypt', 'me_role_saudiarabia', 'me_role_turkey','me_role_iran','me_role_israel',\n",
    "           'fav_sisi','fav_erdogan','fav_assad','fav_netanyahu','fav_salman','fav_rouhani','fav_abdullahii','refugee_iraqsyr',\n",
    "            'war_syria_length', 'fav_adtlpolcnty_rousseff', 'fav_adtlpolcnty_luiz', 'fav_adtlpolcnty_temer',\n",
    "            'fav_lopez', 'fav_radonski', 'fav_allup', 'fav_pri','fav_pan', 'fav_morena', 'fav_prd','fav_modi',\n",
    "            'fav_kejriwal', 'fav_bjp', 'fav_inc','isr_pal_coexist', 'jewish_settlements', 'd_numcell', 'kashmir_military', \n",
    "            'influence_humanrightsorgs', 'nafta_goodbad', 'qsplit',  'racethn', 'racecmb', 'me_role_us', 'prob_kashmir',\n",
    "           'd_density', 'receive_money', 'concern_country', 'humanrights_motive']\n",
    "    \n",
    "}\n",
    "\n",
    "# to reduce names, all partial (or sets) of columns can be added here\n",
    "# if the column name contains any part of this value it will be removed \n",
    "drop_inc_all = ['partyfav', \"d_income\", \"d_race\", 'd_ethnicity', \"d_ptyid_\", \"d_educ_\", \"d_relig_\"\n",
    "                'american_', 'language', 'pray', \"abortion\", \"covid\", 'ladder', \n",
    "               't.sample', 'homephone', 'confid_johnson', 'confid_macron','confid_merkel', 'confid_castro', 'confid_abe',\n",
    "               'confid_modi', 'd_hhcell', 'fav_eu', 'fav_un', 'fav_iran', 'fav_nato', 'fav_india', 'fav_japan', \n",
    "               'fav_ep', 'fav_ec', 'd_working_cell']\n",
    "\n",
    "drop_inc = {\n",
    "    2021 : ['usbest_', 'conflict_', 'climate_intl', 'discrimprob_'], \n",
    "    2020 : ['sdlkjafsldjflakjsdlfjl'],\n",
    "    2019 : ['multiparty', 'churches_', 'language_home', 'ukr_lang', 'brexit_', 'religion20yr',  \n",
    "           'equal_'], # testing without 'id_'\n",
    "    2018 : ['qs6', 'qs7', 'qs8', 'qs9', 'qs10', 'qs11', 'cregion', 'robjob4', 'whymove', \n",
    "            'fiveyears_', 'indiaus', 'eu_', 'cyberattack_', '20yr', 'planmove', 'modern_educ', 'friends_', \n",
    "           'officials_', 'pray_', 'relbehavior', 'pairs_'], \n",
    "    2017 : ['brexit_', 'cell_12months', 'church_', 'trump_', 'obama_', 'mfollow_', 'brexit_policy', 'eu_', 'defend_', \n",
    "            'smartphone', 'textfreq', 'turkey_', 'maduro', 'nieto', 'mex_', 'gandhi', 'modi', 'putin_', 'duterte', \n",
    "           'phil', 'italy_pride', 'stayintouch', 'friends_', 'pray_', 'd_tenure', 'qs6', 'qs7', 'qs8',\n",
    "            'qs9', 'qs10', 'qs11', 'nkorea', 'd_relig_practice', 'humanrights_priority_'], \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure not to drop these varaibles, dispite their missing-ness \n",
    "# highly valuable\n",
    "protected_vars = ['econ_ties_china', 'china_us_enemy', 'us_mil_asia', 'close_relationship',\n",
    "                  'global_money', 'global_information', 'china_jobloss', 'china_deficit', \n",
    "                  'china_taiwan', 'china_environ', 'china_debt', 'china_terrdisputes', 'us_def_china', \n",
    "                  'us_world_role', 'world_role_China', 'world_role_russia', 'usrel_betterworse', 'involved_US', \n",
    "                  'influaffairs_russia', 'interest_surveycountry',\n",
    "                  'china_tough', 'china_tough_econ', 'influaffairs_china'\n",
    "                 ]\n",
    "\n",
    "protected_vars.extend([x + '_categorical' for x in only_categorical_vars])\n",
    "\n",
    "for year in years: \n",
    "    sizeInit = d[year].shape[1]\n",
    "    \n",
    "    # ensure that the rest of the data has proper visibility\n",
    "    # Note 6/2 originally removed with percent of left over data. Manually selected questions moving forward. \n",
    "    for i in d[year].columns:\n",
    "        try:\n",
    "            number = d[year][i].value_counts()[\"Don't know\"]\n",
    "            if (number > (d[year].shape[0] * .9)) & ~(i in protected_vars): \n",
    "                drop[year].append(i)\n",
    "        except KeyError: \n",
    "            # do nothing\n",
    "            number = 0 \n",
    "            \n",
    "    # drop all listed and size-constrained variables \n",
    "    d[year] = d[year].drop(columns=drop[year])\n",
    "    for x in drop_inc[year]: \n",
    "        d[year] = d[year].drop([col for col in d[year].columns if x in col], axis=1)\n",
    "    for x in drop_inc_all: \n",
    "        d[year] = d[year].drop([col for col in d[year].columns if x in col], axis=1)\n",
    "\n",
    "    sizeEnd = d[year].shape[1]\n",
    "    print(\"The data from year \" + str(year) + \" was reduced by \" + str(sizeInit - sizeEnd) + \" columns.\")\n",
    "    print(\"The data is now \" + str(sizeEnd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __We remove all categorical variables in the dataset through dummy variable transformations.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing of all categorical variables to be connected \n",
    "\n",
    "found = {\n",
    "    2021: [],\n",
    "    2020: [], \n",
    "    2019: [], \n",
    "    2018: [], \n",
    "    2017: []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_var(dataset, variable_name, mapping, found): \n",
    "    for year in years: \n",
    "        if variable_name in dataset[year].columns: \n",
    "            print(variable_name + ' variable found in ' + str(year))\n",
    "            dummy_demo = pd.get_dummies(dataset[year][variable_name].map(mapping))\n",
    "            found[year].extend(list(dummy_demo.columns))\n",
    "            \n",
    "            # need to merge dummy variables into df \n",
    "            dataset[year] = pd.concat([dataset[year], dummy_demo], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some variables are categorical \n",
    "# they will be transformed into dummy variables and their original label will be removed \n",
    "\n",
    "# econ_power\n",
    "econ_power_mapping = {\n",
    "    \"The United States\": \"us_econ_power\", \n",
    "    \"China\": \"china_econ_power\",\n",
    "    \"Japan\": \"japan_econ_power\",\n",
    "    \"The countries of the European Union\": \"eu_econ_power\",\n",
    "    \"(VOL) None / There is no leading economic power\": \"no_econ_power\",\n",
    "    \"None / There is no leading economic power (DO NOT READ)\" : \"no_econ_power\",\n",
    "    \"(VOL) Other\": \"other_econ_power\", \n",
    "    \"Other (DO NOT READ)\" : \"other_econ_power\"\n",
    "}\n",
    "\n",
    "# maps onto dummy variables\n",
    "create_dummy_var(d, 'econ_power', econ_power_mapping, found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'us_or_china'\n",
    "\n",
    "econ_us_china = {\n",
    "    \"The United States\" : \"prefer_us_econ\", \n",
    "    \"China\" : \"prefer_china_econ\", \n",
    "    \"Economic ties to both countries are equally important (DO NOT READ)\" : \"both_china_econ\"\n",
    "}\n",
    "\n",
    "create_dummy_var(d, 'us_or_china', econ_us_china, found)\n",
    "for year in years: \n",
    "    if 'both_china_econ' in d[year].columns: \n",
    "        d[year]['prefer_us_econ'] = d[year]['both_china_econ'] + d[year]['prefer_us_econ']\n",
    "        d[year]['prefer_china_econ'] = d[year]['both_china_econ'] + d[year]['prefer_china_econ']\n",
    "        d[year].drop(columns=['both_china_econ', 'no_econ_power', 'other_econ_power'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# econ_power\n",
    "world_leader_mapping = {\n",
    "    \"The U.S. is the world’s leading power\": \"US_better_worldleader\", \n",
    "    \"China is the world’s leading power\": \"China_better_worldleader\",\n",
    "    \"Both (DO NOT READ)\": \"both_better_worldleader\"\n",
    "}\n",
    "\n",
    "# maps onto dummy variables\n",
    "create_dummy_var(d, 'worldleader_uschina', world_leader_mapping, found)\n",
    "for year in years: \n",
    "    if 'both_better_worldleader' in d[year].columns: \n",
    "        d[year]['US_better_worldleader'] = d[year]['both_better_worldleader'] + d[year]['US_better_worldleader']\n",
    "        d[year]['China_better_worldleader'] = d[year]['both_better_worldleader'] + d[year]['China_better_worldleader']\n",
    "        d[year].drop(columns='both_better_worldleader')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify mapping for transformations\n",
    "- Create a list for variables where no transformations are needed \n",
    "- Create all mappings for variables\n",
    "- Search through all variables and map those with similar corresponding labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These column values don't need to be transformed, but we do want to keep them in the dataset \n",
    "# They are either discrete values or they are regional/naming conventions. \n",
    "\n",
    "\n",
    "keep = ['id', 'country', 'sex', 'age', \"d_density\", 'd_hhpeople', 'political_scale2', 'qdate_s', 'qdate_e', \n",
    "        'd_adults', \"d_density\", 'muslim_branch', 'political_affiliation', 'religious_affiliation', \n",
    "        'education_level', 'wealthy', 'regional_location', 'd_adult_us', 'worldleader_uschina']\n",
    "\n",
    "# make sure to add in the categorical vars to use later on\n",
    "keep.extend([x + \"_categorical\" for x in categorical_vars])\n",
    "keep.extend([x + \"_categorical\" for x in only_categorical_vars])\n",
    "\n",
    "keep_inc = {\n",
    "    2021 : [ 'id'], \n",
    "    2020 : [ 'state_us', 'china_us_enemy'],\n",
    "    2019 : [ \"allies_new_1\", \"threats_new_1\", 'close_relationship'], \n",
    "    2018 : [ 'qlang', 'influaffairs_china'], \n",
    "    2017 : [ 'qlang'], \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vars = {\n",
    "    \"Don't know\": 8, \n",
    "    \"Refused\" : 9\n",
    "}\n",
    "\n",
    "sat_bin = {\n",
    "    \"Dissatisfied\": 0, \n",
    "    \"Satisfied\": 1, \n",
    "}\n",
    "\n",
    "sat_q = {\n",
    "    \"Not too satisfied\": 2, \n",
    "    \"Not at all satisfied\" : 1, \n",
    "    \"Somewhat satisfied\": 3, \n",
    "    \"Very satisfied\": 4, \n",
    "}\n",
    "\n",
    "good_bad_q = {\n",
    "    \"Somewhat good\" : 3,\n",
    "    \"Somewhat bad\" : 2,\n",
    "    \"Very bad\" : 1,\n",
    "    \"Very good\" : 4\n",
    "}\n",
    "\n",
    "better_t = {\n",
    "    \"Worse off\" : 1, \n",
    "    \"Gotten worse\" : 1,\n",
    "    \"Worse\" : 1,\n",
    "    \"Better off\" : 3, \n",
    "    \"Better\" : 3, \n",
    "    \"Gotten better\" : 3, \n",
    "    \"Same (DO NOT READ)\" : 2, \n",
    "    \"Stayed about the same\" : 2, \n",
    "    \"About the same\" : 2\n",
    "}\n",
    "\n",
    "prob_q = {\n",
    "    \"Very big problem\" : 4, \n",
    "    \"Moderately big problem\" : 3, \n",
    "    \"Small problem\" : 2, \n",
    "    \"Not a problem at all\" : 1, \n",
    "}\n",
    "\n",
    "fav_q = {\n",
    "    \"Somewhat favorable\" : 3, \n",
    "    \"Somewhat unfavorable\" : 2, \n",
    "    \"Very favorable\": 4, \n",
    "    \"Very unfavorable\" : 1\n",
    "}\n",
    "\n",
    "amount_q = {\n",
    "    \"Great deal\" : 4, \n",
    "    \"A great deal\" : 4,\n",
    "    \"Fair amount\" : 3, \n",
    "    \"A fair amount\" : 3,\n",
    "    \"Not too much\" : 2, \n",
    "    \"Not at all\" : 1\n",
    "}\n",
    "\n",
    "approval_q = {\n",
    "    \"Approve\": 3, \n",
    "    \"Strongly approve\": 4, \n",
    "    \"Disapprove\" : 2, \n",
    "    \"Strongly disapprove\" : 1\n",
    "} \n",
    "\n",
    "confid_q = {\n",
    "    \"No confidence at all\" : 1, \n",
    "    \"Not too much confidence\" : 2, \n",
    "    \"Some confidence\" : 3, \n",
    "    \"A lot of confidence\" : 4\n",
    "}\n",
    "\n",
    "right_t = {\n",
    "    \"About right\" : 2, \n",
    "    \"Too great\" : 3, \n",
    "    \"Too small\" : 1\n",
    "}\n",
    "\n",
    "yesno_bin = {\n",
    "    \"No\" : 0, \n",
    "    \"Yes\" : 1\n",
    "}\n",
    "\n",
    "influe_q = {\n",
    "    \"Great deal of influence\" : 4,\n",
    "    \"Very good influence\" : 4, \n",
    "    \"Fair amount of influence\" : 3, \n",
    "    \"Good influence\" : 3, \n",
    "    \"Bad influence\" : 2, \n",
    "    \"Very bad influence\" : 1, \n",
    "    \"Not too much influence\" : 2, \n",
    "    \"No influence at all\" : 1, \n",
    "    \"No influence (DO NOT READ)\" : 8\n",
    "}\n",
    "  \n",
    "mil_bin = {\n",
    "    \"Yes, would use military force\" : 1, \n",
    "    \"Yes, should use military force\" : 1, \n",
    "    \"No, would not use military force\" : 0, \n",
    "    \"No, should not use military force\" : 0\n",
    "}\n",
    "\n",
    "import_q = {\n",
    "    \"Very important\" : 4,  \n",
    "    \"Somewhat important\" : 3,\n",
    "    \"Not very important\" : 2,\n",
    "    \"Not too important\" :2, \n",
    "    \"Not at all important\": 1, \n",
    "    \"Not important at all\": 1  \n",
    "}\n",
    "\n",
    "roles_t = {\n",
    "    \"More important role\" : 3, \n",
    "    \"Doing more\" : 3, \n",
    "    \"Less important role\" : 1, \n",
    "    \"Doing less\" : 1, \n",
    "    \"As important as 10 years ago\" : 2, \n",
    "    \"U.S. does not help (DO NOT READ)\" : 2, \n",
    "    \"About the same\" : 2\n",
    "}\n",
    "\n",
    "threat_t = {\n",
    "    \"Major threat\" : 3, \n",
    "    \"Not a threat\" : 1, \n",
    "    \"Minor threat\" : 2, \n",
    "    'Very concerned' : 3,\n",
    "    'Very serious' : 3, \n",
    "    'Somewhat concerned' : 2, \n",
    "    'Somewhat serious' : 2, \n",
    "    'Not too serious' : 2, \n",
    "    'Not too concerned' : 2, \n",
    "    'Not at all concerned' : 1, \n",
    "    'Not a problem' : 1\n",
    "}\n",
    "\n",
    "threat_t2 = {    \"Threat\" : 1, \n",
    "    \"Not a threat\" : 0}\n",
    "\n",
    "changes_t = {\n",
    "    'It needs to be completely reformed' : 4, \n",
    "    'It needs major changes' : 3, \n",
    "    'It needs minor changes' : 2, \n",
    "    'It doesn’t need to be changed' : 1\n",
    "}\n",
    "\n",
    "god_bin = {\n",
    "    \"It is necessary to believe in God in order to be moral and have good values\" : 1, \n",
    "    \"It is not necessary to believe in God in order to be moral and have good values\" : 0\n",
    "}\n",
    "\n",
    "china_bin = {\n",
    "    \"The U.S. should try to promote human rights in China, even if it harms economic relations with China\" : 1,\n",
    "    '(response in COUNTRY) should try to promote human rights in China, even if it harms econo' : 1, \n",
    "    \"The U.S. should prioritize strengthening economic relations with China, even if it means not addressing human rights iss\" : 0,\n",
    "    '(response in COUNTRY) should prioritize strengthening economic relations with China, even if it means not addressing' : 0\n",
    "}\n",
    "\n",
    "priority_q = {\n",
    "    \"Top priority\" : 4, \n",
    "    \"Important but lower priority\" : 3, \n",
    "    \"Not too important\" : 2, \n",
    "    \"Should not work on this issue\" : 1\n",
    "}\n",
    "\n",
    "trust_bin = {\n",
    "    \"In general, most people can be trusted\" : 1,\n",
    "    \"In general, most people cannot be trusted\" :0\n",
    "}\n",
    "\n",
    "econ_q = {\n",
    "    \"Improve a lot\" : 5, \n",
    "    \"Improve a little\" : 4,\n",
    "    \"Worsen a little\" : 2,\n",
    "    \"Worsen a lot\" :1, \n",
    "    \"Remain the same\" : 3\n",
    "}\n",
    "\n",
    "trust_q = {\n",
    "    \"A lot\" : 4, \n",
    "    \"Somewhat\" : 3, \n",
    "    \"Not much\" : 2, \n",
    "    \"Not at all\" : 1\n",
    "}\n",
    "\n",
    "enemy_t = {\n",
    "    \"Competitor\" : 2,     \n",
    "    \"Enemy\" :3,\n",
    "    \"Partner\" :1\n",
    "}\n",
    "\n",
    "agree_q = {\n",
    "    \"Mostly disagree\" : 2, \n",
    "    \"Completely disagree\" :1, \n",
    "    \"Mostly agree\" : 3, \n",
    "    \"Completely agree\" : 4, \n",
    "}\n",
    "\n",
    "goodbad_b = {\n",
    "    \"Bad thing\" : 0, \n",
    "    \"Good thing\" : 1, \n",
    "    \"Neither good nor bad\" : 0, \n",
    "    \"Neither (DO NOT READ)\" : 0,\n",
    "    \"Both (DO NOT READ)\" :0\n",
    "}\n",
    "\n",
    "goodbad_b2 = {\n",
    "    \"Investment from China is a good thing\" : 1, \n",
    "    \"Investment from China is a bad thing\" : 0\n",
    "}\n",
    "\n",
    "posneg_b  = {\n",
    "    \"Positive\" : 1, \n",
    "    \"Negative\" : 0, \n",
    "    \"Neither/both (DO NOT READ)\" : 0   \n",
    "}\n",
    "\n",
    "opto_b = {\n",
    "    \"Optimistic\" : 1, \n",
    "    \"Pessimistic\" : 0, \n",
    "    \"Neither (DO NOT READ)\" : 0\n",
    "}\n",
    "\n",
    "smart_b = {\n",
    "    \"Smartphone\" : 1,\n",
    "    \"Not a smartphone\" : 0\n",
    "}\n",
    "\n",
    "cell_b = {\n",
    "    \"Yes, someone in household has cell phone\" : 1, \n",
    "    \"No\" : 0 \n",
    "}\n",
    "\n",
    "global_b = {\n",
    "    \"should act as part of a global community that works together to solve problems\" : 1,                      \n",
    "    \"should act as independent nations that compete with other countries and pursue their own interests\" : 0, \n",
    "    \"Both (DO NOT READ)\" : 1, \n",
    "    \"Neither (DO NOT READ)\" : 0                  \n",
    "}\n",
    "\n",
    "homo_b = {\n",
    "    \"Homosexuality should be accepted by society\" : 1,\n",
    "    \"Homosexuality should not be accepted by society\" : 0,  \n",
    "}\n",
    "\n",
    "relat_b = {\n",
    "    \"Building a strong relationship with China on economic issues\" : 0,\n",
    "    \"Getting tougher with China on economic issues\" : 1\n",
    "}\n",
    "\n",
    "news_q = {\n",
    "    \"Very well\" : 4, \n",
    "    \"Somewhat well\" :3, \n",
    "    \"Not too well\" : 2, \n",
    "    \"Not well at all\" : 1, \n",
    "    \"News organizations should not do this (DO NOT READ)\" : 0\n",
    "}\n",
    "\n",
    "news_b = {\n",
    "    \"It is never acceptable for a news organization to favor one political party over others when reporting news\" : 0, \n",
    "    \"It is sometimes acceptable for a news organization to favor one political party over others when reporting news\" : 1\n",
    "}\n",
    "\n",
    "respect_b = {\n",
    "    \"Yes, respects personal freedoms\" : 1, \n",
    "    \"No, does not respect personal freedoms\" : 0 \n",
    "}\n",
    "\n",
    "support_b = {\n",
    "    \"Support\" : 1, \n",
    "    \"Oppose\" : 0\n",
    "}\n",
    "    \n",
    "civic_q = {\n",
    "    \"Have done in the past year\" : 4, \n",
    "    \"Have done in the more distant past\" : 3, \n",
    "    \"Have not done, but might do\" : 2, \n",
    "    \"Have not done and would never, under any circumstances, do\" : 1\n",
    "}\n",
    "\n",
    "increase_t = {\n",
    "    \"Increase\" : 3, \n",
    "    \"Decrease\" : 1,\n",
    "    \"Does not make a difference\" : 2\n",
    "}\n",
    "\n",
    "nukes_t = {\n",
    "    \"Too much\" : 3, \n",
    "    \"About what needs to be done OR\" : 2, \n",
    "    \"Too little\" : 1\n",
    "}\n",
    "\n",
    "jobs_t = {\n",
    "    \"Job creation\" : 3, \n",
    "    \"Job losses\" : 1, \n",
    "    \"Does not make a difference\" : 2\n",
    "}\n",
    "\n",
    "likely_q = {\n",
    "    \"Very likely\" : 4, \n",
    "    \"Somewhat likely\" : 3, \n",
    "    \"Not too likely\" : 2, \n",
    "    \"Not at all likely\" : 1\n",
    "}\n",
    "\n",
    "social_s = {\n",
    "    \"Several times a day\" : 7,  \n",
    "    \"Once a day\" : 6,       \n",
    "    \"Several times a week\" : 5, \n",
    "    \"Once a week\" : 4, \n",
    "    \"Several times a month\" : 3, \n",
    "    \"Once a month\" : 2, \n",
    "    \"Less than once a month\" : 1,  \n",
    "    \"Never\" : 0,                    \n",
    "}\n",
    "\n",
    "better_place_t = {\n",
    "    \"A better place to live\" : 3, \n",
    "    \"A worse place to live\" : 1, \n",
    "    \"Doesn\\'t make much difference either way (DO NOT READ)\" : 2\n",
    "}\n",
    "\n",
    "reliability = {\n",
    "    \"Very reliable\" : 4,\n",
    "    \"Somewhat reliable\" : 3, \n",
    "    \"Not too reliable\" : 2,\n",
    "    \"Not at all reliable\" : 1, \n",
    "}\n",
    "\n",
    "dictionaries = [sat_bin, good_bad_q, sat_q, better_t, fav_q, amount_q, approval_q, confid_q, right_t, \n",
    "               yesno_bin, influe_q, mil_bin, import_q, roles_t, threat_t, prob_q, trust_q, threat_t2]\n",
    "\n",
    "dictionaries_niche = [god_bin, trust_bin, china_bin, econ_q, enemy_t, agree_q, goodbad_b, goodbad_b2,  \n",
    "                      posneg_b, opto_b, smart_b, global_b, homo_b, relat_b, news_q, respect_b, support_b, \n",
    "                     civic_q, increase_t, nukes_t, jobs_t, likely_q, social_s, priority_q, news_b, \n",
    "                     cell_b, better_place_t, reliability, changes_t]\n",
    "\n",
    "# ensures the missing variables are included in the datasets \n",
    "\n",
    "for i in dictionaries: \n",
    "    i.update(missing_vars)\n",
    "    \n",
    "for i in dictionaries_niche: \n",
    "    i.update(missing_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function provides a matching mechanism for data labels into a numeric scale \n",
    "# This scale is constant across years (where positive responses are ranked highest)\n",
    "# The original dataset is overrridden with these transformations \n",
    "def preprocess(year, dictionaries, found):\n",
    "    \n",
    "    for i in d[year].columns: \n",
    "        if ('qs' not in i) and ('region' not in i) and (i not in protected_vars) and (i not in keep) and (i not in keep_inc[year]) and (i not in found[year]): \n",
    "            for di in dictionaries: \n",
    "                if len(set(d[year][i]).difference(set(di.keys()))) == 0: \n",
    "                    found[year].append(i)\n",
    "                    d[year][i] = d[year][i].map(di)\n",
    "                    break\n",
    "                       \n",
    "            for di in dictionaries_niche: \n",
    "                if len(set(d[year][i]).difference(set(di.keys()))) == 0: \n",
    "                    found[year].append(i)\n",
    "                    d[year][i] = d[year][i].map(di)\n",
    "                    break\n",
    "                       \n",
    "        else: \n",
    "            found[year].append(i)\n",
    "                \n",
    "    return d[year], found[year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE 2021 columns \"reliable_us\" --> \"relations_us\" and \"climate_concern\" --> \"intthreat_climatechange\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years: \n",
    "    \n",
    "    print(\"Currently, we are preprocessing year \" + str(year))\n",
    "    d[year], found[year] = preprocess(year, dictionaries, found)\n",
    "    print(\"There were \" + str(len(found[year])) + \" columns preprocessed.\")\n",
    "    print(\"This means that there were \" + str(len(set(d[year].columns).difference(set(found[year])))) + \" columns left to support: \")\n",
    "    print(set(d[year].columns).difference(set(found[year])))\n",
    "    print(\"\")\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Commonalities \n",
    "\n",
    "Through individual parsing of data, it is possible that variables denoting the same question with different variable names have been listed as seperate columns in the merged dataset. We look to identify any variables of identical questions that were NOT merged accordingly and manually adjust the final spreadsheet. \n",
    "\n",
    "Currently, we have a secondary sheet logged within each original file which contains the actual questioned asked of respondants. Our goal is to identify similarity between these questions, we then can confirm these mappings and then transform the overlapping variables to the same name. Here is the methodology: \n",
    "\n",
    "- Create a mapping between the question variable name and the question itself for each variable in every year. \n",
    "- ~~Use WordMoverDistance to identify semantic similarities~~\n",
    "- Provide a listing of variables with the questions listed for approval \n",
    "- Verified pairs will be listed in a dataframe \n",
    "- All verified pairs will be replaced with a common variable name \n",
    "\n",
    "Then we can return to merging our data together. \n",
    "\n",
    "*Note: This process was instead conducted through manually matching our variables of interest to save time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of a map to transform variable names to a common entity\n",
    "\n",
    "varTransformAll = {\n",
    "    'children_betteroff2' : 'children_betteroff', \n",
    "    'id' : 'id_survey'\n",
    "}\n",
    "varTransform = {\n",
    "    2021: {'d_adults' : 'd_adult_us', \n",
    "           'confid_biden' : 'confid_uspres', \n",
    "           'reliable_us' : 'us_relation', \n",
    "           'polsys_country' : 'polsys_reform', \n",
    "          }, \n",
    "    2020: {'confid_trump' : 'confid_uspres', \n",
    "           'china_tough_categorical' : 'china_tough_econ_categorical'\n",
    "          }, \n",
    "    2019: {'confid_trump' : 'confid_uspres', \n",
    "           'relations_us' : 'us_relation', \n",
    "           'allies_new_1' : 'foreign_allies', \n",
    "           'threats_new_1' : 'foreign_threats', \n",
    "           'china_influ_econ' : 'china_influence', \n",
    "           'intthreat_uspower' : 'us_threat', \n",
    "           'intthreat_chpower' : 'china_threat'\n",
    "          },\n",
    "    2018: {'confid_trump' : 'confid_uspres', \n",
    "           'influaffairs_china' : 'china_influence',\n",
    "           'intthreat_uspower' : 'us_threat', \n",
    "           'intthreat_chpower' : 'china_threat'\n",
    "          },\n",
    "    2017: {'confid_trump' : 'confid_uspres',\n",
    "           'intthreat_uspower' : 'us_threat', \n",
    "           'intthreat_chpower' : 'china_threat'\n",
    "          }\n",
    "}\n",
    "\n",
    "for year in years: \n",
    "    d[year].rename(columns=varTransformAll, inplace=True)\n",
    "    d[year].rename(columns=varTransform[year], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c21 = set(d[2021].columns)\n",
    "c20 = set(d[2020].columns)\n",
    "c19 = set(d[2019].columns)\n",
    "c18 = set(d[2018].columns)\n",
    "c17 = set(d[2017].columns)\n",
    "#assert len(var_set) == 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c21.intersection(c20).intersection(c19).intersection(c18).intersection(c17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Merge - reducing dimensionality \n",
    "\n",
    "- each year has a time frame added when the survey was conducted \n",
    "- data is merged on like column names \n",
    "- dimensionality reduction showcased "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the survey year to the dataframe \n",
    "for year in years: \n",
    "    d[year]['survey_year'] = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessing data dimensionality \n",
    "count = 0\n",
    "col = 0\n",
    "for year in years: \n",
    "    count = d[year].shape[0] + count\n",
    "    col = d[year].shape[1] + col\n",
    "    print(\"The dimensionality of this year is \" + str(count) + \" by \" + str(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data merge \n",
    "\n",
    "df = pd.DataFrame()\n",
    "for year in years: \n",
    "    df = df.append(d[year].reset_index())\n",
    "    \n",
    "df = df.reset_index()\n",
    "print(\"The dimensionality of the combined data is \" + str(df.shape[0]) + \" by \" + str(df.shape[1]))\n",
    "print(\"In total, we have captured \" + str(round(count * 100 / df.shape[0], 2)) + \"% of the data after the merge.\")\n",
    "print(\"There has been a \" + str(round((col - df.shape[1]) * 100 / df.shape[1], 2)) + \"% decrease in column through overlapping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Across all of our years of data \" + str(df.dropna(axis=1).shape[1]) + \" columns are present across the dataset. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranformations\n",
    "\n",
    "For consistency, conduct your final transformations prior to export. This includes mapping all religious affiliations to categories. \n",
    "\n",
    "Note: consider bridging over political affiliations as well. Requires lots of research on political groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation for all religious beliefs. \n",
    "# If the label CONTAINS the key, it should be grouped into the value categories. \n",
    "relig_transform  = {\n",
    "    r\"(.*)Christian(.*)\" : \"Christian\", \n",
    "    r\"(.*)Unitarian(.*)\" : \"Christian\", \n",
    "    r\"(.*)Agnostic(.*)\" : \"Agnostic\", \n",
    "    r\"(.*)African(.*)\" : \"traditional African religion\", \n",
    "    r\"(.*)Atheist(.*)\" : \"Atheist\", \n",
    "    r\"(.*)Baha(.*)\" : \"Bahai\", \n",
    "    r\"(.*)Buddhis(.*)\" : \"Buddhist\",\n",
    "    r\"(.*)Buddist(.*)\" : \"Buddhist\", \n",
    "    r\"(.*)Catholic(.*)\" : \"Catholic\", \n",
    "    r\"(.*)Confucianism(.*)\" : \"Confucianism\", \n",
    "    r\"(.*)Congregationalist(.*)\" : \"Protestant\", \n",
    "    r\"(.*)Druze(.*)\" : \"Druze\", \n",
    "    r\"(.*)Evangelical(.*)\" : \"Protestant\", \n",
    "    r\"(.*)Hindu(.*)\" : \"Hindu\", \n",
    "    r\"(.*)Iglesia ni Cristo(.*)\" : \"Christian\", \n",
    "    r\"(.*)Indigenous religion(.*)\" : \"Indigenous religion\", \n",
    "    r\"(.*)Jain(.*)\" : \"Jain\", \n",
    "    r\"(.*)Jehova(.*)\" : \"Restorationist Christian\",  \n",
    "    r\"(.*)Jew(.*)\" : \"Jewish\", \n",
    "    r\"(.*)Lutheran(.*)\" : \"Protestant\", \n",
    "    r\"(.*)Mormon(.*)\" : \"Restorationist Christian\", \n",
    "    r\"(.*)Muslim(.*)\" : \"Muslim\", \n",
    "    r\"(.*)No(.*)\" : None, \n",
    "    r\"(.*)Orthodox(.*)\" : \"Catholic\", \n",
    "    r\"(.*)Pentecostal(.*)\" : \"Protestant\", \n",
    "    r\"(.*)Presbyterian(.*)\" : \"Protestant\", \n",
    "    r\"(.*)Protestant(.*)\" : \"Protestant\", \n",
    "    r\"(.*)Sikh(.*)\" : \"Sikh\", \n",
    "    r\"(.*)Something else(.*)\" : \"religious\",\n",
    "    r\"(.*)Spiritist(.*)\" : \"Spiritist\", \n",
    "    r\"(.*)Refused(.*)\" : None,\n",
    "    r\"(.*)Afrobrazilian religion(.*)\" : \"Afrobrazilian religion\", \n",
    "    r\"(.*)Unification(.*)\" : \"Christian\", \n",
    "    r\"(.*)Unitarian(.*)\" : \"Christian\", \n",
    "    r\"(.*)Yes(.*)\" : \"religious\", \n",
    "}\n",
    "\n",
    "df['religious_affiliation'] = df['religious_affiliation'].replace(regex=relig_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidating needed variables \n",
    "\n",
    "We have now cleaned the whole dataset. Let's now make it applicable to China. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['level_0', 'index'], inplace=True)\n",
    "df['id'] = np.arange(1, len(df) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_vars = ['survey_year', 'country', 'id', 'id_survey', 'regional_location', 'qdate_s', 'sex', 'male', 'age', 'd_hhpeople', \n",
    "               'd_adult_us', 'wealthy', 'political_affiliation', 'religious_affiliation', 'education_level', \n",
    "               'us_econ_power', 'china_econ_power', 'japan_econ_power', 'eu_econ_power', \n",
    "               'other_econ_power', 'prefer_us_econ', 'prefer_china_econ', 'children_betteroff', \n",
    "               'confid_uspres_categorical', 'confid_uspres', 'us_threat', 'china_threat'] \n",
    "\n",
    "categorical_vars.remove('confid_biden')\n",
    "categorical_vars.remove('confid_trump')\n",
    "add_categorical = [x + '_categorical' for x in categorical_vars]\n",
    "categorical_vars.remove('intthreat_uspower')\n",
    "categorical_vars.remove('intthreat_chpower')\n",
    "\n",
    "only_categorical_vars.remove('china_tough')\n",
    "add_only_categorical = [x + '_categorical' for x in only_categorical_vars]\n",
    "categorical_vars.remove('children_betteroff2')\n",
    "categorical_vars.remove('econ_power')\n",
    "categorical_vars.remove('us_or_china')\n",
    "\n",
    "needed_vars.extend(categorical_vars)\n",
    "needed_vars.extend(add_categorical)\n",
    "needed_vars.extend(add_only_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean d_adult_us\n",
    "\n",
    "adult_us = {\n",
    "    \"Don't know\" : 88,\n",
    "    \"Don’t know\" : 88,\n",
    "    \"8 or more\" : 8, \n",
    "    \"9\" : 8, \n",
    "    \"11\" : 8, \n",
    "    \"10\" : 8, \n",
    "    \"15\" : 8,\n",
    "    \"12\" : 8, \n",
    "    \"2.65415019762846\" : 3, \n",
    "    \"2.80942828485456\" : 3,\n",
    "    \"4.06118355065196\" : 4,\n",
    "    \"Refused\" : 88, \n",
    "    \"97+\" : 100\n",
    "}\n",
    "\n",
    "df['d_adult_us'] = [int(x) for x in df['d_adult_us'].replace(adult_us)]\n",
    "df['d_hhpeople'] = [int(x) if pd.notna(x) else x for x in df['d_hhpeople'].replace(adult_us)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the country variable for 2021 data to be gathered before geocoding\n",
    "\n",
    "country_map = {\n",
    "    \"singapore\" : \"Singapore\", \n",
    "    \"greece\" : \"Greece\", \n",
    "    \"sweden\" : \"Sweden\", \n",
    "    \"japan\" : \"Japan\", \n",
    "    \"germany_2017\" : \"Germany\",\n",
    "    \"canada_2017\" : \"Canada\", \n",
    "    \"skorea_2017\" : \"South Korea\", \n",
    "    \"italy_2017\" : \"Italy\", \n",
    "    \"uk_2017\" : \"United Kingdom\", \n",
    "    \"spain\" : \"Spain\", \n",
    "    \"taiwan\" : \"Taiwan\", \n",
    "    \"australia_2017\" : \"Australia\", \n",
    "    \"newzealand\" : \"New Zealand\", \n",
    "    \"belgium\" : \"Belgium\",\n",
    "    \"france\" : \"France\" , \n",
    "    \"netherlands\" : \"Netherlands\", \n",
    "    \"germany_vocational\" : \"Germany\", \n",
    "    \"australia_2017a\" : \"Australia\"\n",
    "}\n",
    "\n",
    "s = dOriginal[2021].filter(regex=(\"(d_educ_.*)\"))\n",
    "s.columns= s.columns.str.split(\"d_educ_\", expand=True)\n",
    "\n",
    "s = (s.replace(\"Don't know\", np.nan))\n",
    "\n",
    "s = s.stack().reset_index().drop_duplicates(subset='level_0')\n",
    "\n",
    "dict_country = dict(zip(s['level_0'], s['level_1']))\n",
    "\n",
    "df['temp_country'] = df['id'].map(dict_country).replace(country_map)\n",
    "\n",
    "df['country'] = [x if pd.isna(y) else y for x, y in zip(df['country'], df['temp_country'])]\n",
    "\n",
    "# Note: still need to geocode countries and regions in seperate notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform sex to be able to be used in modeling \n",
    "\n",
    "males = {\n",
    "    'Male' : 1, \n",
    "    'Female' : 0\n",
    "}\n",
    "\n",
    "df['male'] = df['sex'].map(males)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform d_hhpeople for 2021 due to missing data\n",
    "\n",
    "# calculating the average difference between entire households and just adults. \n",
    "t = df.loc[(df['survey_year'] < 2021) & (df['d_hhpeople'] <15) & (df['d_adult_us'] <15)]\n",
    "t['diff'] = (t['d_hhpeople'] - t['d_adult_us'])\n",
    "\n",
    "# from the results, we can infer how many children are in household \n",
    "print(t[t['diff'] > 0]['diff'].mean())\n",
    "\n",
    "# then we add this delta (2) to the d_hhpeople variable \n",
    "# any grouping larger than 8 will be categorized as 8 and understood as '8+'\n",
    "df['d_hhpeople'] = [int(x + 2) if (pd.isna(y)) & (x != 88) else (x if (x == 88) else int(y)) for x, y in zip(df['d_adult_us'], df['d_hhpeople'])]\n",
    "df['d_hhpeople'] = [x if (x < 9) | (x == 88) else 8 for x in df['d_hhpeople']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping for econ power categorical variable \n",
    "\n",
    "econ_power_mapping = {\n",
    "    \"(VOL) None / There is no leading economic power\": \"There is no leading economic power\", \n",
    "    \"None / There is no leading economic power (DO NOT READ)\" : \"There is no leading economic power\",\n",
    "    'None/There is no leading economic power (DO NOT READ)' : \"There is no leading economic power\",\n",
    "    \"(VOL) Other\": \"There is an economic power (not China, US, Japan, or EU)\", \n",
    "    \"Other (DO NOT READ)\" : \"There is an economic power (not China, US, Japan, or EU)\", \n",
    "    \"Other (DO NOT  READ)\" : \"There is an economic power (not China, US, Japan, or EU)\", \n",
    "    'Refused' : 'Refused to answer', \n",
    "    '(VOL)\\xa0None / There is no leading economic power' : \"There is no leading economic power\"\n",
    "}\n",
    "\n",
    "df['econ_power_categorical'] = df['econ_power_categorical'].replace(econ_power_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['us_or_china_categorical'] = [x.replace(r\" (DO NOT READ)\", \"\") if pd.notna(x) else x for x in df['us_or_china_categorical']]\n",
    "\n",
    "us_or_china_map = {\n",
    "    'Economic ties to both countries\\xa0are equally important' : 'Economic ties to both countries are equally important', \n",
    "    'Neither' : 'Economic ties to neither country is important'\n",
    "}\n",
    "\n",
    "df['us_or_china_categorical'] = df['us_or_china_categorical'].replace(us_or_china_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergeing categorical results for US presidents \n",
    "\n",
    "df['confid_uspres_categorical'] = [x if pd.notna(x) else y for x, y in zip(df['confid_biden_categorical'], df['confid_trump_categorical'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unneeded labels in categorical variables \n",
    "\n",
    "df['children_betteroff2_categorical'] = [x.replace(\" (DO NOT READ)\", \"\") if pd.notna(x) else x for x in df['children_betteroff2_categorical']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to the supportive variables \n",
    "df = df[needed_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'threats_new_1_categorical':'foreign_threats_categorical', 'allies_new_1_categorical':'foreign_allies_categorical', 'close_relationship_categorical': 'russia_relationship_categorical'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all the questions left as categorical responses are ready to be mapped over \n",
    "\n",
    "# read in the dictionary of variables to provide the mapping \n",
    "var_list = pd.read_excel(\"pewQVDict.xlsx\", sheet_name='Final Variable Listing')\n",
    "\n",
    "# build the dictionary \n",
    "\n",
    "mini_vars = var_list[pd.notna(var_list['categorical'])]\n",
    "keys = [x + '_categorical' for x in mini_vars['question']]\n",
    "\n",
    "dictionary = dict(zip(keys, mini_vars['variable_name']))\n",
    "\n",
    "# map over the variables to the new names \n",
    "df = df.rename(columns=dictionary)\n",
    "\n",
    "# remove labels that contain the phrase '(DO NOT READ)'\n",
    "df = df.replace('\\(DO NOT READ\\)', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a couple of last mappings to clean up the categorical responses\n",
    "df['china_econ_military: Does China\\'s economic or military strength concern you more?'] = df['china_econ_military: Does China\\'s economic or military strength concern you more?'].replace({'Its economic strength [OR]' : 'Its economic strength'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point in time, only geocoded variables should be left\n",
    "set(var_list['variable_name']).difference(set(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = pd.read_excel(\"pewQVDict.xlsx\", sheet_name='Final Variable Listing')\n",
    "len(var_list['variable_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data \n",
    "\n",
    "Export data to begin geocoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"pew_processed.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix \n",
    "\n",
    "Throughout the preprocessing of this data, several areas of expansion were identified. These include: \n",
    "- __parsing of political affililation__ - data currently includes favorability to 'mainstream' parties and party affiliation. Transformation to leaning across countries could be valuable. Currently, all political identification moved toward one generic variable \"political_party\". \n",
    "- __updating income level__ - there is a variable 'd_income2_' that supposedly categorizes wealth. This label is inaccurate and doesn't log all of respondants wealth, regardless of a variable 'd_income_' that has this granularity. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
